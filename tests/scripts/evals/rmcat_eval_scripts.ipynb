{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from shutil import copyfile\n",
    "import os.path\n",
    "\n",
    "def get_packetlog_columns():\n",
    "    return [ 'extended_seq',\n",
    "             'tracked_seq',\n",
    "             'state',\n",
    "             'tracked_ntp',\n",
    "             'marker',\n",
    "             'header_size',\n",
    "             'payload_size',\n",
    "             'timestamp',\n",
    "             'payload_type',\n",
    "             'played_out'];\n",
    "\n",
    "\n",
    "def get_delta_packetlog_columns():\n",
    "    return [\"owd\",\n",
    "            \"BiF\",\n",
    "            \"playout_delay\"];\n",
    "\n",
    "def get_statslog_columns():\n",
    "    return [ 'rcved_tot_packets',\n",
    "             'rcved_tot_bytes',\n",
    "             'rcved_acc_bytes',\n",
    "             'rcved_acc_packets',                             \n",
    "\n",
    "             'lost_tot_packets',\n",
    "             'lost_tot_bytes',\n",
    "             'lost_acc_bytes',\n",
    "             'lost_acc_packets',\n",
    "\n",
    "             'discarded_tot_packets',\n",
    "             'discarded_tot_bytes',\n",
    "             'discarded_acc_bytes',\n",
    "             'discarded_acc_packets',\n",
    "\n",
    "             'corrupted_tot_packets',\n",
    "             'corrupted_tot_bytes',\n",
    "             'corrupted_acc_bytes',\n",
    "             'corrupted_acc_packets',\n",
    "\n",
    "             'repaired_tot_packets',\n",
    "             'repaired_tot_bytes',\n",
    "             'repaired_acc_bytes',\n",
    "             'repaired_acc_packets',\n",
    "\n",
    "             'fec_tot_packets',\n",
    "             'fec_tot_bytes',\n",
    "             'fec_acc_bytes',\n",
    "             'fec_acc_packets',\n",
    "            ];\n",
    "def get_veth_columns():\n",
    "    return [\"tcp_bytes\",\n",
    "            \"tcp_packets\",\n",
    "            \"tcp_flowsnum\",\n",
    "            \"bw_forward\", \n",
    "            \"bw_backward\"];\n",
    "\n",
    "def get_statlog_columns():\n",
    "    return [ ];\n",
    "\n",
    "def csv_append(touched, resultfile, obj):\n",
    "    if(resultfile in touched):\n",
    "        #print(resultfile + \" is already touched\")\n",
    "        ;\n",
    "    else:\n",
    "#         print(resultfile + \" is removed\")\n",
    "        !rm $resultfile\n",
    "        touched[resultfile] = True\n",
    "        \n",
    "    if(os.path.isfile(resultfile) == False) : \n",
    "        with open(resultfile, 'w') as f:\n",
    "            writer = csv.writer(f)\n",
    "            writer.writerow(obj.keys())\n",
    "\n",
    "    # print(\"------ Obj to save to \" + resultfile + \"------\")\n",
    "    with open(resultfile, 'a') as f:\n",
    "        dict_writer = csv.DictWriter(f, obj.keys(), \"\")\n",
    "        dict_writer.writerow(obj)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from shutil import copyfile\n",
    "import os.path\n",
    "import pandas as pd\n",
    "import datetime\n",
    "\n",
    "#Make owd_statlogs\n",
    "def make_merged_packetlogs(snd_packetlogs, rcv_packetlogs, merged_packetlogs_path):\n",
    "    packetlog_columns     = get_packetlog_columns()\n",
    "    snd_packets = pd.read_csv(snd_packetlogs, names=packetlog_columns)\n",
    "    rcv_packets = pd.read_csv(rcv_packetlogs, names=packetlog_columns)\n",
    "    \n",
    "    result = pd.merge(snd_packets, rcv_packets, on='extended_seq', \\\n",
    "                           suffixes=['_snd','_rcv'])\n",
    "\n",
    "    result.to_csv(merged_packetlogs_path, sep=',')\n",
    "    \n",
    "    #Make owd_statlogs\n",
    "def make_path_statlogs(veth_logfile, \n",
    "#                        targets_logfile,\n",
    "                       snd_statlogfile, \n",
    "                       rcv_statlogfile,\n",
    "                       delta_packetlogfile, \n",
    "                       result_filepath,\n",
    "                       bytefactor):\n",
    "    \n",
    "    statlog_columns     = get_statslog_columns()\n",
    "    \n",
    "    delta_columns = get_delta_packetlog_columns()\n",
    "   \n",
    "    #targets_stats = pd.read_csv(targets_logfile,     names=[\"target\"])\n",
    "    veth_stats    = pd.read_csv(veth_logfile,        names=get_veth_columns())\n",
    "    snd_stats     = pd.read_csv(snd_statlogfile,     names=statlog_columns)\n",
    "    rcv_stats     = pd.read_csv(rcv_statlogfile,     names=statlog_columns)\n",
    "    packet_dstats = pd.read_csv(delta_packetlogfile, names=delta_columns)\n",
    "    \n",
    "    sending_rates = snd_stats.apply(lambda record: record['rcved_acc_packets'] * bytefactor + \n",
    "                                    record['rcved_acc_bytes'], axis=1)\n",
    "    # sending_rates = snd_stats.apply(lambda record: record['rcved_acc_bytes'], axis=1)\n",
    "    \n",
    "    fec_rates     = snd_stats.apply(lambda record: record['fec_acc_packets'] * bytefactor + \n",
    "                                    record['fec_acc_bytes'], axis=1)\n",
    "    \n",
    "    ffre          = rcv_stats.apply(lambda record:  \n",
    "                                    record['repaired_acc_packets'] / (record['lost_acc_packets']) if 0 < record['lost_acc_packets'] else 0, axis=1)\n",
    "    \n",
    "    #print(rcv_stats['lost_acc_packets'])\n",
    "    #print(rcv_stats['repaired_acc_packets'])\n",
    "    #print(packet_dstats);\n",
    "    result = pd.concat([veth_stats[\"bw_forward\"], \n",
    "                        veth_stats[\"bw_backward\"], \n",
    "                        sending_rates,\n",
    "                        fec_rates,\n",
    "                        packet_dstats['owd'],\n",
    "                        packet_dstats['BiF'],\n",
    "                        ffre,\n",
    "#                         targets_stats[\"target\"],\n",
    "                        packet_dstats['playout_delay'],\n",
    "                        rcv_stats[\"lost_acc_packets\"],\n",
    "                        rcv_stats[\"repaired_acc_packets\"],\n",
    "                        veth_stats[\"tcp_flowsnum\"],\n",
    "                        veth_stats[\"tcp_bytes\"],\n",
    "                        veth_stats[\"tcp_packets\"],                        \n",
    "                       ], \n",
    "                       axis=1)\n",
    "#    print(result)\n",
    "    result.to_csv(result_filepath, sep=',')\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from shutil import copyfile\n",
    "from collections import OrderedDict\n",
    "import os.path\n",
    "import pandas as pd\n",
    "import datetime\n",
    "\n",
    "def evaluate_statslog(statslog_file, \n",
    "                      packetlogs_file, \n",
    "                      veth_logfile, \n",
    "                      dpck_logfile,\n",
    "                      sampling_num, \n",
    "                      bytefactor):\n",
    "    \n",
    "    packetlog_columns = get_packetlog_columns()\n",
    "    statlog_table     = pd.read_csv(statslog_file, names=get_statslog_columns())\n",
    "    veth_table        = pd.read_csv(veth_logfile,  names=[\"tcps\",\"bw\"])\n",
    "    dpcklog_table     = pd.read_csv(dpck_logfile,  names=get_delta_packetlog_columns()).head(sampling_num + 1).tail(sampling_num)\n",
    "    goodput_helper    = pd.Series(((statlog_table[\"rcved_acc_bytes\"] - statlog_table[\"discarded_acc_bytes\"]) + \n",
    "                                 (statlog_table[\"rcved_acc_packets\"] - statlog_table[\"discarded_acc_packets\"]) * bytefactor ) / 125)\n",
    "    \n",
    "    owdmean_helper    = pd.Series(dpcklog_table[\"owd\"] / 1000)\n",
    "\n",
    "    #shaping\n",
    "    available_sampling_num = len(statlog_table[\"rcved_acc_packets\"])\n",
    "    if(available_sampling_num < sampling_num) : \n",
    "        print(\"Required sampling number is \" + str(sampling_num) + \n",
    "              \" available: \" + str(available_sampling_num) + \" at \" + statslog_file)\n",
    "        return\n",
    "    \n",
    "    # print(\"Required sampling num is \" + str(sampling_num) + \" : \" + str(len(statlog_table[\"rcved_acc_packets\"])))\n",
    "    \n",
    "    statlog_table     = statlog_table.head(sampling_num + 1).tail(sampling_num)\n",
    "    veth_table        = veth_table.head(sampling_num + 1).tail(sampling_num)\n",
    "    dpcklog_table     = dpcklog_table.head(sampling_num + 1).tail(sampling_num)\n",
    "    goodput_helper    = goodput_helper.head(sampling_num + 1).tail(sampling_num)\n",
    "\n",
    "    index = sampling_num - 1\n",
    "    rcved_packets     = statlog_table.iloc[index][\"rcved_tot_packets\"]\n",
    "    repaired_packets  = statlog_table.iloc[index][\"repaired_tot_packets\"]\n",
    "    rcved_packets     = statlog_table.iloc[index][\"rcved_tot_packets\"]\n",
    "    rcved_bytes       = statlog_table.iloc[index][\"rcved_tot_bytes\"]\n",
    "    discarded_packets = statlog_table.iloc[index][\"discarded_tot_packets\"]\n",
    "    discarded_bytes   = statlog_table.iloc[index][\"discarded_tot_bytes\"]\n",
    "    lost_packets      = statlog_table.iloc[index][\"lost_tot_packets\"]\n",
    "    received_packets  = statlog_table.iloc[index][\"rcved_tot_packets\"]\n",
    "    lost_rate         = float(lost_packets) / float(received_packets + lost_packets)\n",
    "    lost_rate         = round(lost_rate*100,2)\n",
    "    \n",
    "    packetlogs_table  = pd.read_csv(packetlogs_file, names=packetlog_columns).head(received_packets + 1).tail(received_packets)\n",
    "    \n",
    "\n",
    "    frame_is_lost   = False\n",
    "    lost_frames     = 0\n",
    "    received_frames = 0    \n",
    "    last_timestamp  = 0;\n",
    "    \n",
    "    for index, row in packetlogs_table.iterrows():\n",
    "        if(row[\"timestamp\"] != 0):\n",
    "            last_timestamp = row[\"timestamp\"]\n",
    "            break\n",
    "\n",
    "    for index, row in packetlogs_table.iterrows():\n",
    "        if(row[\"timestamp\"] == 0 or row[\"state\"] != 1):\n",
    "            frame_is_lost = True;\n",
    "            continue;\n",
    "        if(row[\"timestamp\"] != last_timestamp):\n",
    "            last_timestamp = row[\"timestamp\"]\n",
    "            if(frame_is_lost):\n",
    "                lost_frames += 1\n",
    "            else: \n",
    "                received_frames += 1\n",
    "            frame_is_lost = False;\n",
    "            \n",
    "    repaired_rate = 0.\n",
    "    if(0 < lost_packets):\n",
    "        repaired_rate = float(repaired_packets) / float(repaired_packets + lost_packets)\n",
    "    repaired_rate = round(repaired_rate*100,2)\n",
    "    \n",
    "    lost_frame_rate = 0;\n",
    "    if(0 < received_frames):\n",
    "        lost_frame_rate = float(lost_frames) / float(lost_frames + received_frames)\n",
    "    lost_frame_rate = round(lost_frame_rate*100,2)\n",
    "    \n",
    "    return OrderedDict([\n",
    "        ('repaired_packets_num',    repaired_packets),\n",
    "        ('repaired_rate',           repaired_rate),\n",
    "        ('lost_packet_rate',        lost_rate), \n",
    "        ('goodput_mean',            round(goodput_helper.mean(), 2)),\n",
    "        ('goodput_std',             round(goodput_helper.std(), 2)),\n",
    "        ('lost_packets_num',        lost_packets),\n",
    "        ('rcved_packets_num',       rcved_packets),\n",
    "        ('lost_frames_num',         lost_frames),\n",
    "        ('lost_frame_rate',         lost_frame_rate),\n",
    "        ('received_frames',         received_frames),\n",
    "        ('owd mean',                owdmean_helper.mean()),\n",
    "        ('owd std',                 owdmean_helper.std()),\n",
    "        ('received_bytes',          rcved_bytes),\n",
    "    ]);\n",
    "\n",
    "def evaluate_videofiles(original, processed, width, height, framesnum, targetname):\n",
    "    psnr_file   = targetname + \"_psnr.csv\"\n",
    "    ssim_file   = targetname + \"_ssim.csv\"\n",
    "    msssim_file = targetname + \"_msssim.csv\"\n",
    "    vifp_file   = targetname + \"_vifp.csv\"\n",
    "    columns     = [\"frame\",\"value\"]\n",
    "    \n",
    "    print(\"Evaluating Video\")\n",
    "    print(\"Source: \" + original +\" Processed: \" + processed + \" W: \" + str(width) + \" H: \" + str(height) + \" psnr: \" + psnr_file)\n",
    "    print(\"Framesnum: \" + str(framesnum))\n",
    "    \n",
    "    #!vqmt $original $processed $height $width $framesnum 1 $targetname PSNR SSIM MSSSIM VIFP\n",
    "    !vqmt $original $processed $height $width $framesnum 1 $targetname PSNR \n",
    "    \n",
    "    psnr    = pd.Series(pd.read_csv(psnr_file,    names=columns).head(-1).tail(-1).astype('float')[\"value\"])\n",
    "    #ssim    = pd.Series(pd.read_csv(ssim_file,    names=columns).head(-1).tail(-1).astype('float')[\"value\"])\n",
    "    #msssim  = pd.Series(pd.read_csv(msssim_file,  names=columns).head(-1).tail(-1).astype('float')[\"value\"])\n",
    "    #vifp    = pd.Series(pd.read_csv(vifp_file,    names=columns).head(-1).tail(-1).astype('float')[\"value\"])\n",
    "    \n",
    "    return {\n",
    "        \"psnr_mean\": psnr.mean(),\n",
    "        \"psnr_std\":  psnr.std(),\n",
    "        #\"ssim_mean\": ssim.mean(),\n",
    "        #\"ssim_std\":  ssim.std(),\n",
    "        #\"msssim_mean\": msssim.mean(),\n",
    "        #\"msssim_std\":  msssim.std(),\n",
    "        #\"vifp_mean\": vifp.mean(),\n",
    "        #\"vifp_std\":  vifp.std(),\n",
    "        };\n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#!cd ../../temp; ls #The directory where the logfiles are\n",
    "base_path       = \"../../\"\n",
    "plot_path       = \"../gnuplots\"\n",
    "temp_path       = base_path + \"/temp\" \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--- Operate in ../../scripts/saves/rmcat3/fractal_10_100ms_0ms ---\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import csv\n",
    "\n",
    "# - - - - - - RMCA1 Config - - - - - - - - - -\n",
    "# test              = \"rmcat1\"\n",
    "# flownum           = 1\n",
    "# min_samplingnum   = 1200\n",
    "\n",
    "# - - - - - - RMCA2 Config - - - - - - - - - -\n",
    "# test              = \"rmcat2\"\n",
    "# flownum           = 2\n",
    "# min_samplingnum   = 1200\n",
    "\n",
    "# - - - - - - RMCA3 Config - - - - - - - - - -\n",
    "test              = \"rmcat3\"\n",
    "flownum           = 2\n",
    "min_samplingnum   = 1200\n",
    "\n",
    "# - - - - - - RMCA4 Config - - - - - - - - - -\n",
    "# test              = \"rmcat4\"\n",
    "# flownum           = 3\n",
    "# min_samplingnum   = 1200\n",
    "\n",
    "# - - - - - - RMCAT5 Config - - - - - - - - - -\n",
    "# test              = \"rmcat5\"\n",
    "# flownum           = 3\n",
    "# min_samplingnum   = 2800\n",
    "\n",
    "# - - - - - - RMCAT6 Config - - - - - - - - - -\n",
    "# test              = \"rmcat6\"\n",
    "# flownum           = 1\n",
    "# min_samplingnum   = 1200\n",
    "\n",
    "# # - - - - - - RMCAT7 Config - - - - - - - - - -\n",
    "# test              = \"rmcat7\"\n",
    "# flownum           = 1\n",
    "# min_samplingnum   = 2900\n",
    "\n",
    "\n",
    "test_path         = base_path + \"scripts/saves/\" + test + \"/\"\n",
    "touched           = {}\n",
    "flowstat_suffixes = [\"\",\"2\",\"3\"]\n",
    "flowcsv_prefixes  = [\"flow1\", \"flow2\", \"flow3\"]\n",
    "\n",
    "viewpdf           = base_path + \"/temp/statlogs.pdf\"\n",
    "\n",
    "\n",
    "\n",
    "for dirname, dirnames, filenames in os.walk(test_path):\n",
    "    for targetdir in dirnames:\n",
    "        algorithm, count, owd, jitter = targetdir.split('_')\n",
    "        meas_path = base_path + \"scripts/saves/\" + test + \"/\" + '_'.join([algorithm, count, owd, jitter])\n",
    "        \n",
    "        #Ignoring some cases if we know that they are ok.\n",
    "#         if(\n",
    "#            algorithm != \"fractal\" or\n",
    "#            owd != \"50ms\"        or\n",
    "#            count != \"10\"          or\n",
    "#            0\n",
    "#           ):\n",
    "#             print(meas_path + \"is ignored\")\n",
    "#             continue;\n",
    "            \n",
    "        print(\"--- Operate in \" + meas_path + \" ---\")\n",
    "            \n",
    "        bandwidth        = meas_path + \"/bandwidth.csv\"\n",
    "        veth0_stats      = meas_path + \"/veth0_stats.csv\"\n",
    "        rmcat_tmplogs    = meas_path + \"/rmcat_tmplogs.csv\"\n",
    "        plotfile         = plot_path + \"/\" + test + \".plot\"\n",
    "        outplot          = meas_path + \"/\" + '_'.join([test,algorithm,owd,jitter]) + \".pdf\"\n",
    "        gnuplot_params   = \"\"\n",
    "        bytefactor       = 28 if algorithm == \"fractal\" else 20\n",
    "        \n",
    "        # print(\"Bytefactor: \" + str(bytefactor))\n",
    "        \n",
    "        for flowcounter in range(0, flownum):\n",
    "            \n",
    "            stat_suffix      = flowstat_suffixes[flowcounter]\n",
    "            \n",
    "            snd_statlogs     = meas_path + \"/snd_statlogs\"    + stat_suffix + \".csv\"\n",
    "            rcv_statlogs     = meas_path + \"/rcv_statlogs\"    + stat_suffix + \".csv\"\n",
    "            snd_packetlogs   = meas_path + \"/snd_packetlogs\"  + stat_suffix + \".csv\"\n",
    "            rcv_packetlogs   = meas_path + \"/rcv_packetlogs\"  + stat_suffix + \".csv\"\n",
    "            merged_statlogs  = meas_path + \"/merged_statlogs\" + stat_suffix + \".csv\"  \n",
    "            delta_statlogs   = meas_path + \"/delta_statlogs\"  + stat_suffix + \".csv\"\n",
    "            rmcat_statlogs   = meas_path + \"/rmcat_statlogs\"  + stat_suffix + \".csv\"\n",
    "            evalfile         = meas_path + \"/evaluation\"      + stat_suffix + \".csv\"\n",
    "\n",
    "            gnuplot_params  += \"-e statlogs=\\'\" + rmcat_statlogs + \"\\' \"\n",
    "            \n",
    "            #Made merged statlogs\n",
    "            make_merged_packetlogs(snd_packetlogs, rcv_packetlogs, merged_statlogs)\n",
    "            !./../../make_delta_statlogs $merged_statlogs $delta_statlogs > log.txt\n",
    "\n",
    "            make_path_statlogs(veth0_stats, \n",
    "            #              targetsfile, \n",
    "                           snd_statlogs, \n",
    "                           rcv_statlogs, \n",
    "                           delta_statlogs, \n",
    "                           rmcat_statlogs,\n",
    "                           bytefactor)\n",
    "\n",
    "            !sed '1d' $rmcat_statlogs > $rmcat_tmplogs; mv $rmcat_tmplogs $rmcat_statlogs\n",
    "            \n",
    "            flowcsv_prefix = flowcsv_prefixes[flowcounter]\n",
    "            \n",
    "            eval_result = evaluate_statslog(rcv_statlogs, \n",
    "                                            rcv_packetlogs, \n",
    "                                            veth0_stats, \n",
    "                                            delta_statlogs, \n",
    "                                            min_samplingnum,\n",
    "                                            bytefactor)\n",
    "            \n",
    "            psnr_resultfile = meas_path + \"/vqmt_psnr.csv\"\n",
    "            if(os.path.isfile( psnr_resultfile )) : \n",
    "                psnr = pd.Series(pd.read_csv(psnr_resultfile, names=[\"frame\",\"value\"]).head(-1).tail(-1).astype('float')[\"value\"])\n",
    "                eval_result[\"psnr_mean\"] = psnr.mean()\n",
    "                eval_result[\"psnr_std\"]  = psnr.std()\n",
    "                \n",
    "            resultfile = test_path + '_'.join([flowcsv_prefix, algorithm, owd, jitter]) +\".csv\"\n",
    "            csv_append(touched, resultfile, eval_result)\n",
    "        \n",
    "        if(flownum == 1):\n",
    "            !gnuplot -e \"statlogs='$meas_path/rmcat_statlogs.csv'\" \\\n",
    "                 -e \"output_file='$outplot'\" \\\n",
    "                 $plotfile\n",
    "        elif(flownum == 2):\n",
    "            !gnuplot -e \"statlogs='$meas_path/rmcat_statlogs.csv'\" \\\n",
    "                     -e \"statlogs2='$meas_path/rmcat_statlogs2.csv'\" \\\n",
    "                     -e \"output_file='$outplot'\" \\\n",
    "                     $plotfile\n",
    "        elif(flownum == 3):\n",
    "            !gnuplot -e \"statlogs='$meas_path/rmcat_statlogs.csv'\" \\\n",
    "                     -e \"statlogs2='$meas_path/rmcat_statlogs2.csv'\" \\\n",
    "                     -e \"statlogs3='$meas_path/rmcat_statlogs3.csv'\" \\\n",
    "                     -e \"output_file='$outplot'\" \\\n",
    "                     $plotfile\n",
    "\n",
    "#                 for run in range(start, 10):\n",
    "        \n",
    "# for resultfile in touched:\n",
    "#     !echo \"$resultfile\" >> $allinonecsv\n",
    "#     !cat $resultfile >> $allinonecsv\n",
    "#     print(resultfile + \" is added to \" + allinonecsv)\n",
    "\n",
    "\n",
    "\n",
    "algorithms = [\"fractal\", \"scream\"]\n",
    "owds       = [\"50ms\", \"100ms\", \"300ms\",\"50x100x150ms\"]\n",
    "jitters    = [\"0ms\"]\n",
    "\n",
    "with open(test_path + \"all.csv\", 'wb') as csvfile:\n",
    "    writeCSV = csv.writer(csvfile)\n",
    "    for alg in algorithms:\n",
    "        for owd in owds:\n",
    "            for jitter in jitters:\n",
    "                for flowcounter in range(0, flownum):\n",
    "                    flow_prefix = flowcsv_prefixes[flowcounter]\n",
    "                    csv_path = test_path + '_'.join([flow_prefix, alg, owd, jitter]) + \".csv\"\n",
    "                    if(os.path.isfile(csv_path) == False):\n",
    "                        continue\n",
    "                    start = 0\n",
    "                    with open(csv_path) as csvfile:\n",
    "                        readCSV = csv.reader(csvfile, delimiter=',')\n",
    "                        info = \" | \".join([\"algorithm: \" + alg, \n",
    "                                           \"owd: \"       + owd, \n",
    "                                           \"jitter: \"    + jitter, \n",
    "                                           \"flow: \"      + flow_prefix])\n",
    "                        writeCSV.writerow([info])\n",
    "                        for row in readCSV:\n",
    "                            writeCSV.writerow(row)\n",
    "                            start+=1\n",
    "                        for emptyRow in range(start, 11):\n",
    "                            writeCSV.writerow('')\n",
    "                        print(info + \" processed, \" + str(11 - start) + \" row is added as empty\")\n",
    "\n",
    "                        \n",
    "print(\"Process Done\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1\n",
      "2\n",
      "3\n",
      "4\n",
      "5\n",
      "6\n",
      "7\n",
      "8\n",
      "9\n",
      "10\n",
      "11\n",
      "algorithm: fractal | owd: 50ms | jitter: 0ms | flow: flow1 processed, 0 row is added as empty\n",
      "1\n",
      "2\n",
      "3\n",
      "4\n",
      "5\n",
      "6\n",
      "7\n",
      "8\n",
      "9\n",
      "10\n",
      "11\n",
      "algorithm: fractal | owd: 50ms | jitter: 0ms | flow: flow2 processed, 0 row is added as empty\n",
      "algorithm: fractal | owd: 100ms | jitter: 0ms | flow: flow1 processed, 11 row is added as empty\n",
      "1\n",
      "2\n",
      "3\n",
      "4\n",
      "5\n",
      "algorithm: fractal | owd: 100ms | jitter: 0ms | flow: flow2 processed, 6 row is added as empty\n",
      "1\n",
      "2\n",
      "algorithm: fractal | owd: 300ms | jitter: 0ms | flow: flow1 processed, 9 row is added as empty\n",
      "1\n",
      "2\n",
      "algorithm: fractal | owd: 300ms | jitter: 0ms | flow: flow2 processed, 9 row is added as empty\n"
     ]
    },
    {
     "ename": "IOError",
     "evalue": "[Errno 2] No such file or directory: '../../scripts/saves/rmcat2/flow1_fractal_50x100x150ms_0ms.csv'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mIOError\u001b[0m                                   Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-19-1bd77363008a>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      8\u001b[0m                     \u001b[0mcsv_path\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtest_path\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0;34m'_'\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mjoin\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mflow_prefix\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0malg\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mowd\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mjitter\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0;34m\".csv\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      9\u001b[0m                     \u001b[0mstart\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 10\u001b[0;31m                     \u001b[0;32mwith\u001b[0m \u001b[0mopen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcsv_path\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mcsvfile\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     11\u001b[0m                         \u001b[0mreadCSV\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcsv\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mreader\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcsvfile\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdelimiter\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m','\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     12\u001b[0m                         info = \" | \".join([\"algorithm: \" + alg, \n",
      "\u001b[0;31mIOError\u001b[0m: [Errno 2] No such file or directory: '../../scripts/saves/rmcat2/flow1_fractal_50x100x150ms_0ms.csv'"
     ]
    }
   ],
   "source": [
    "with open(test_path + \"all.csv\", 'wb') as csvfile:\n",
    "    writeCSV = csv.writer(csvfile)\n",
    "    for alg in algorithms:\n",
    "        for owd in owds:\n",
    "            for jitter in jitters:\n",
    "                for flowcounter in range(0, flownum):\n",
    "                    flow_prefix = flowcsv_prefixes[flowcounter]\n",
    "                    csv_path = test_path + '_'.join([flow_prefix, alg, owd, jitter]) + \".csv\"\n",
    "                    start = 0\n",
    "                    with open(csv_path) as csvfile:\n",
    "                        readCSV = csv.reader(csvfile, delimiter=',')\n",
    "                        info = \" | \".join([\"algorithm: \" + alg, \n",
    "                                           \"owd: \"       + owd, \n",
    "                                           \"jitter: \"    + jitter, \n",
    "                                           \"flow: \"      + flow_prefix])\n",
    "                        writeCSV.writerow([info])\n",
    "                        for row in readCSV:\n",
    "                            writeCSV.writerow(row)\n",
    "                            start += 1\n",
    "                            print(start)\n",
    "                        for emptyRow in range(start, 11):\n",
    "                            writeCSV.writerow('')\n",
    "                        print(info + \" processed, \" + str(11 - start) + \" row is added as empty\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#Save measurement results\n",
    "import csv\n",
    "\n",
    "# test      = \"rmcat1\"\n",
    "# test      = \"rmcat2\"\n",
    "# test      = \"rmcat3\"\n",
    "# test      = \"rmcat4\"\n",
    "test      = \"rmcat5\"\n",
    "# algorithm = \"fractal\"\n",
    "algorithm = \"scream\"\n",
    "count     = \"1\"\n",
    "# owd       = \"300ms\" \n",
    "# owd      = \"100ms\" \n",
    "# owd      = \"50ms\" \n",
    "owd      = \"50x100x150ms\"\n",
    "jitter    = \"0ms\"\n",
    "target    = \"_\".join([algorithm, count, owd, jitter])\n",
    "\n",
    "#For analyzing videoresult\n",
    "# evaluate_videofiles(base_path + \"produced.yuv\", \n",
    "#                     base_path + \"consumed.yuv\", \n",
    "#                     352,  #width\n",
    "#                     288,  #height\n",
    "#                     2000, #frames\n",
    "#                     temp_path + \"/vqmt\")\n",
    "\n",
    "save_destination = base_path + \"scripts/saves/\" + test + \"/\" + target\n",
    "if(os.path.exists(save_destination) == False): \n",
    "    !mkdir $save_destination\n",
    "!cp $temp_path/* $save_destination\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
