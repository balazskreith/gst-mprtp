{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Test evaluation subsystem for gst-mprtp\n",
    "\n",
    "All of the test, which has the snd\\_ and rcv\\_ packetlogs and statlogs along with the veth logs evaluable. You need to define the necessary definitions found in the second and third cell and then in the configuration section you need to run the evaluate measurements process by the appropriate configuration parameters.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# from shutil import copyfile\n",
    "import os.path\n",
    "\n",
    "def get_packetlog_columns():\n",
    "    return [ 'extended_seq',\n",
    "             'tracked_seq',\n",
    "             'state',\n",
    "             'tracked_ntp',\n",
    "             'marker',\n",
    "             'header_size',\n",
    "             'payload_size',\n",
    "             'timestamp',\n",
    "             'payload_type',\n",
    "             'played_out'];\n",
    "\n",
    "\n",
    "def get_delta_packetlog_columns():\n",
    "    return [\"owd\",\n",
    "            \"BiF\",\n",
    "            \"playout_delay\"];\n",
    "\n",
    "def get_statslog_columns():\n",
    "    return [ 'rcved_tot_packets',\n",
    "             'rcved_tot_bytes',\n",
    "             'rcved_acc_bytes',\n",
    "             'rcved_acc_packets',                             \n",
    "\n",
    "             'lost_tot_packets',\n",
    "             'lost_tot_bytes',\n",
    "             'lost_acc_bytes',\n",
    "             'lost_acc_packets',\n",
    "\n",
    "             'discarded_tot_packets',\n",
    "             'discarded_tot_bytes',\n",
    "             'discarded_acc_bytes',\n",
    "             'discarded_acc_packets',\n",
    "\n",
    "             'corrupted_tot_packets',\n",
    "             'corrupted_tot_bytes',\n",
    "             'corrupted_acc_bytes',\n",
    "             'corrupted_acc_packets',\n",
    "\n",
    "             'repaired_tot_packets',\n",
    "             'repaired_tot_bytes',\n",
    "             'repaired_acc_bytes',\n",
    "             'repaired_acc_packets',\n",
    "\n",
    "             'fec_tot_packets',\n",
    "             'fec_tot_bytes',\n",
    "             'fec_acc_bytes',\n",
    "             'fec_acc_packets',\n",
    "            ];\n",
    "def get_veth_columns():\n",
    "    return [\"tcp_bytes\",\n",
    "            \"tcp_packets\",\n",
    "            \"tcp_flowsnum\",\n",
    "            \"bw_forward\", \n",
    "            \"bw_backward\",\n",
    "            \"bw_forward2\",\n",
    "            \"bw_forward3\"];\n",
    "\n",
    "def get_statlog_columns():\n",
    "    return [ ];\n",
    "\n",
    "def csv_append(touched, resultfile, obj):\n",
    "    if(resultfile in touched):\n",
    "        #print(resultfile + \" is already touched\")\n",
    "        ;\n",
    "    else:\n",
    "#         print(resultfile + \" is removed\")\n",
    "        !rm $resultfile\n",
    "        touched[resultfile] = True\n",
    "        \n",
    "    if(os.path.isfile(resultfile) == False) : \n",
    "        with open(resultfile, 'w') as f:\n",
    "            writer = csv.writer(f)\n",
    "            writer.writerow(obj.keys())\n",
    "\n",
    "    # print(\"------ Obj to save to \" + resultfile + \"------\")\n",
    "    with open(resultfile, 'a') as f:\n",
    "        dict_writer = csv.DictWriter(f, obj.keys(), \"\")\n",
    "        dict_writer.writerow(obj)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from shutil import copyfile\n",
    "import os.path\n",
    "import pandas as pd\n",
    "import datetime\n",
    "\n",
    "#Make owd_statlogs\n",
    "def make_merged_packetlogs(snd_packetlogs, rcv_packetlogs, merged_packetlogs_path):\n",
    "    packetlog_columns     = get_packetlog_columns()\n",
    "    snd_packets = pd.read_csv(snd_packetlogs, names=packetlog_columns)\n",
    "    rcv_packets = pd.read_csv(rcv_packetlogs, names=packetlog_columns)\n",
    "    \n",
    "    result = pd.merge(snd_packets, rcv_packets, on='extended_seq', \\\n",
    "                           suffixes=['_snd','_rcv'])\n",
    "\n",
    "    result.to_csv(merged_packetlogs_path, sep=',')\n",
    "    \n",
    "    #Make owd_statlogs\n",
    "def make_path_statlogs(veth_logfile, \n",
    "#                        targets_logfile,\n",
    "                       snd_statlogfile, \n",
    "                       rcv_statlogfile,\n",
    "                       delta_packetlogfile, \n",
    "                       result_filepath,\n",
    "                       bytefactor):\n",
    "    \n",
    "    statlog_columns     = get_statslog_columns()\n",
    "    \n",
    "    delta_columns = get_delta_packetlog_columns()\n",
    "   \n",
    "    #targets_stats = pd.read_csv(targets_logfile,     names=[\"target\"])\n",
    "    veth_stats    = pd.read_csv(veth_logfile,        names=get_veth_columns())\n",
    "    snd_stats     = pd.read_csv(snd_statlogfile,     names=statlog_columns)\n",
    "    rcv_stats     = pd.read_csv(rcv_statlogfile,     names=statlog_columns)\n",
    "    packet_dstats = pd.read_csv(delta_packetlogfile, names=delta_columns)\n",
    "    \n",
    "    sending_rates = snd_stats.apply(lambda record: record['rcved_acc_packets'] * bytefactor + \n",
    "                                    record['rcved_acc_bytes'], axis=1)\n",
    "    # sending_rates = snd_stats.apply(lambda record: record['rcved_acc_bytes'], axis=1)\n",
    "    \n",
    "    fec_rates     = snd_stats.apply(lambda record: record['fec_acc_packets'] * 40 + \n",
    "                                    record['fec_acc_bytes'], axis=1)\n",
    "    \n",
    "    ffre          = rcv_stats.apply(lambda record:  \n",
    "                                    record['repaired_acc_packets'] / (record['lost_acc_packets']) if 0 < record['lost_acc_packets'] else 0, axis=1)\n",
    "    \n",
    "    #print(rcv_stats['lost_acc_packets'])\n",
    "    #print(rcv_stats['repaired_acc_packets'])\n",
    "    #print(packet_dstats);\n",
    "    result = pd.concat([veth_stats[\"bw_forward\"], \n",
    "                        veth_stats[\"bw_backward\"], \n",
    "                        sending_rates,\n",
    "                        fec_rates,\n",
    "                        packet_dstats['owd'],\n",
    "                        packet_dstats['BiF'],\n",
    "                        ffre,\n",
    "#                         targets_stats[\"target\"],\n",
    "                        packet_dstats['playout_delay'],\n",
    "                        rcv_stats[\"lost_acc_packets\"],\n",
    "                        rcv_stats[\"repaired_acc_packets\"],\n",
    "                        veth_stats[\"tcp_flowsnum\"],\n",
    "                        veth_stats[\"tcp_bytes\"],\n",
    "                        veth_stats[\"tcp_packets\"],\n",
    "                        veth_stats[\"bw_forward2\"], \n",
    "                        veth_stats[\"bw_forward3\"], \n",
    "                       ], \n",
    "                       axis=1)\n",
    "#    print(result)\n",
    "    result.to_csv(result_filepath, sep=',')\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from shutil import copyfile\n",
    "from collections import OrderedDict\n",
    "import os.path\n",
    "import pandas as pd\n",
    "import datetime\n",
    "import numpy\n",
    "import collections\n",
    "\n",
    "def evaluate_statslog(snd_statslog_file, \n",
    "                      snd_packetlogs_file,\n",
    "                      rcv_statslog_file, \n",
    "                      rcv_packetlogs_file, \n",
    "                      veth_logfile, \n",
    "                      dpck_logfile,\n",
    "                      sampling_num, \n",
    "                      algorithm,\n",
    "                      total_flownum,\n",
    "                      bytefactor):\n",
    "    \n",
    "    packetlog_columns = get_packetlog_columns()\n",
    "    statlog_table     = pd.read_csv(rcv_statslog_file, names=get_statslog_columns())\n",
    "    snd_statlog_table = pd.read_csv(snd_statslog_file, names=get_statslog_columns())    \n",
    "    \n",
    "    veth_table        = pd.read_csv(veth_logfile,  names=get_veth_columns())\n",
    "        \n",
    "    statlog_table     = statlog_table.join(snd_statlog_table, rsuffix = '_snd', lsuffix = '_rcv')\n",
    "    statlog_table     = statlog_table.join(veth_table)\n",
    "\n",
    "    dpcklog_table     = pd.read_csv(dpck_logfile,  names=get_delta_packetlog_columns()).head(sampling_num + 1).tail(sampling_num)\n",
    "    goodput_helper    = pd.Series(((statlog_table[\"rcved_acc_bytes_rcv\"] - statlog_table[\"discarded_acc_bytes_rcv\"]) + \n",
    "                                 (statlog_table[\"rcved_acc_packets_rcv\"] - statlog_table[\"discarded_acc_packets_rcv\"]) * bytefactor ) / 125)\n",
    "    fecrate_helper   = pd.Series(((statlog_table[\"fec_acc_bytes_snd\"]) + \n",
    "                                  (statlog_table[\"fec_acc_packets_snd\"] * 40)) / 125)\n",
    "    \n",
    "    owdmean_helper    = pd.Series(dpcklog_table[\"owd\"] / 1000)\n",
    "\n",
    "    #shaping\n",
    "    available_sampling_num = len(statlog_table[\"rcved_acc_packets_rcv\"])\n",
    "    if(available_sampling_num < sampling_num) : \n",
    "        print(\"Required sampling number is \" + str(sampling_num) + \n",
    "              \" available: \" + str(available_sampling_num) + \" at \" + rcv_statslog_file)\n",
    "        return\n",
    "    \n",
    "    # print(\"Required sampling num is \" + str(sampling_num) + \" : \" + str(len(statlog_table[\"rcved_acc_packets\"])))\n",
    "    \n",
    "    statlog_table     = statlog_table.head(sampling_num + 1).tail(sampling_num)\n",
    "    veth_table        = veth_table.head(sampling_num + 1).tail(sampling_num)\n",
    "    dpcklog_table     = dpcklog_table.head(sampling_num + 1).tail(sampling_num)\n",
    "    goodput_helper    = goodput_helper.head(sampling_num + 1).tail(sampling_num)\n",
    "    fecrate_helper    = fecrate_helper.head(sampling_num + 1).tail(sampling_num)\n",
    "    \n",
    "    index = sampling_num - 1\n",
    "    rcved_packets     = statlog_table.iloc[index][\"rcved_tot_packets_rcv\"]\n",
    "    repaired_packets  = statlog_table.iloc[index][\"repaired_tot_packets_rcv\"]\n",
    "    rcved_packets     = statlog_table.iloc[index][\"rcved_tot_packets_rcv\"]\n",
    "    rcved_bytes       = statlog_table.iloc[index][\"rcved_tot_bytes_rcv\"]\n",
    "    discarded_packets = statlog_table.iloc[index][\"discarded_tot_packets_rcv\"]\n",
    "    discarded_bytes   = statlog_table.iloc[index][\"discarded_tot_bytes_rcv\"]\n",
    "    lost_packets      = statlog_table.iloc[index][\"lost_tot_packets_rcv\"]\n",
    "    received_packets  = int(statlog_table.iloc[index][\"rcved_tot_packets_rcv\"])\n",
    "    \n",
    "    lost_rate         = float(lost_packets) / float(received_packets + lost_packets) if 0 < lost_packets else 0.\n",
    "    lost_rate         = round(lost_rate*100,2)\n",
    "\n",
    "    #print(\"received_packets: \" + str(received_packets))\n",
    "    packetlogs_table  = pd.read_csv(rcv_packetlogs_file, names=packetlog_columns).head(received_packets + 1).tail(received_packets)\n",
    "    \n",
    "\n",
    "    frame_is_lost   = False\n",
    "    lost_frames     = 0\n",
    "    received_frames = 0    \n",
    "    last_timestamp  = 0;\n",
    "    \n",
    "    for index, row in packetlogs_table.iterrows():\n",
    "        if(row[\"timestamp\"] != 0):\n",
    "            last_timestamp = row[\"timestamp\"]\n",
    "            break\n",
    "\n",
    "    for index, row in packetlogs_table.iterrows():\n",
    "        if(row[\"timestamp\"] == 0 or row[\"state\"] != 1):\n",
    "            frame_is_lost = True;\n",
    "            continue;\n",
    "        if(row[\"timestamp\"] != last_timestamp):\n",
    "            last_timestamp = row[\"timestamp\"]\n",
    "            if(frame_is_lost):\n",
    "                lost_frames += 1\n",
    "            else: \n",
    "                received_frames += 1\n",
    "            frame_is_lost = False;\n",
    "      \n",
    "    repaired_rate = 0.\n",
    "    ffre          = 0.\n",
    "    tfs_list      = []\n",
    "    tfs           = 0\n",
    "\n",
    "    #algorithm cut by fractal and scream\n",
    "    last_tot_fec_packets       = 0 \n",
    "    protected_but_lost         = 0\n",
    "    recovered                  = 0\n",
    "    not_protected_and_lost     = 0\n",
    "    last_row                   = 0\n",
    "    tot_fec_packets = collections.deque()\n",
    "    tot_fec_packets_count      = 0\n",
    "    init = False\n",
    "    for index, row in statlog_table.iterrows():\n",
    "        #calculate ffre\n",
    "        tot_fec_packets.append(row[\"fec_tot_packets_snd\"])\n",
    "        tot_fec_packets_count += 1\n",
    "        if(init == False):\n",
    "            init = True\n",
    "            last_row = row\n",
    "            continue\n",
    "\n",
    "        if(2 < tot_fec_packets_count):\n",
    "            last_tot_fec_packets   = tot_fec_packets.popleft()\n",
    "            tot_fec_packets_count -= 1\n",
    "\n",
    "        last_known_total_lost      = row[\"lost_tot_packets_rcv\"]\n",
    "        last_known_total_recovered = row[\"repaired_tot_packets_rcv\"]\n",
    "        drecovered         = row[\"repaired_tot_packets_rcv\"] - last_row[\"repaired_tot_packets_rcv\"]\n",
    "        dlost              = row[\"lost_tot_packets_rcv\"] - last_row[\"lost_tot_packets_rcv\"] \n",
    "        recovered          += drecovered \n",
    "        if(row[\"fec_tot_packets_snd\"] != last_tot_fec_packets):\n",
    "            protected_but_lost += dlost - drecovered if drecovered <= dlost else 0\n",
    "        else:\n",
    "            not_protected_and_lost += dlost - drecovered if drecovered <= dlost else 0\n",
    "        last_row = row\n",
    "        \n",
    "        #calculate tfs\n",
    "        if(row[\"tcp_flowsnum\"] == 0):\n",
    "            actual_tfs = 0\n",
    "            continue\n",
    "        elif(row[\"rcved_acc_bytes_snd\"] == 0):\n",
    "            print(\"Warning! RTP Sent bytes are 0\")\n",
    "            actual_tfs = 0\n",
    "        else:\n",
    "            tcp_throughput = float(row[\"tcp_bytes\"] + row[\"tcp_packets\"] * 20)            \n",
    "            actual_tfs  =  tcp_throughput / float(row[\"tcp_flowsnum\"])\n",
    "            actual_tfs /= float(tcp_throughput + row[\"rcved_acc_bytes_snd\"] + row[\"rcved_acc_packets_snd\"] * bytefactor) / float(row[\"tcp_flowsnum\"] + 1)\n",
    "        tfs_list.append(actual_tfs)\n",
    "            \n",
    "    #print(\"protected_but_lost: \" + str(protected_but_lost) + \" lost\" + str(not_protected_and_lost) + \" recovered: \" + str(recovered))\n",
    "    #print(\"TFS: \" + str(numpy.array(tfs).mean()))\n",
    "    if(0 < protected_but_lost):\n",
    "        ffre = float(100 * recovered) / float(protected_but_lost + recovered)\n",
    "    if(tfs_list):\n",
    "        tfs = numpy.array(tfs_list).mean() * 100.0\n",
    "        \n",
    "    if(0 < lost_packets):\n",
    "        repaired_rate = float(repaired_packets) / float(repaired_packets + lost_packets)\n",
    "    repaired_rate = round(repaired_rate*100,2)\n",
    "    \n",
    "    lost_frame_rate = 0;\n",
    "    if(0 < received_frames):\n",
    "        lost_frame_rate = float(lost_frames) / float(lost_frames + received_frames)\n",
    "    lost_frame_rate = round(lost_frame_rate*100,2)\n",
    "    \n",
    "    #Handy if you want to identify the measurement not by count but by property\n",
    "    #print(\"The goodput mean is: \" + str(round(goodput_helper.mean(), 2)) )\n",
    "    return OrderedDict([\n",
    "        #('repaired_packets_num',    repaired_packets),\n",
    "        #('repaired_rate',           repaired_rate),\n",
    "        ('fecrate_mean',            fecrate_helper.mean()),\n",
    "        ('fecrate_std',             fecrate_helper.std()), \n",
    "        ('lost_packet_rate',        lost_rate), \n",
    "        ('goodput_mean',            round(goodput_helper.mean(), 2)),\n",
    "        ('goodput_std',             round(goodput_helper.std(), 2)),\n",
    "        #('lost_packets_num',        lost_packets),\n",
    "        ('ffre',                    ffre), \n",
    "        ('rcved_packets_num',       rcved_packets),\n",
    "        ('lost_frames_num',         lost_frames),\n",
    "        ('lost_frame_rate',         lost_frame_rate),\n",
    "        ('received_frames',         received_frames),\n",
    "        ('owd mean',                owdmean_helper.mean()),\n",
    "        ('owd std',                 owdmean_helper.std()),\n",
    "        #('received_bytes',          rcved_bytes), \n",
    "        ('tfs',                     tfs)\n",
    "    ]);\n",
    "\n",
    "def evaluate_videofiles(original, processed, width, height, framesnum, targetname):\n",
    "    psnr_file   = targetname + \"_psnr.csv\"\n",
    "    ssim_file   = targetname + \"_ssim.csv\"\n",
    "    msssim_file = targetname + \"_msssim.csv\"\n",
    "    vifp_file   = targetname + \"_vifp.csv\"\n",
    "    columns     = [\"frame\",\"value\"]\n",
    "    \n",
    "    print(\"Evaluating Video\")\n",
    "    print(\"Source: \" + original +\" Processed: \" + processed + \" W: \" + str(width) + \" H: \" + str(height) + \" psnr: \" + psnr_file)\n",
    "    print(\"Framesnum: \" + str(framesnum))\n",
    "    \n",
    "    #!vqmt $original $processed $height $width $framesnum 1 $targetname PSNR SSIM MSSSIM VIFP\n",
    "    !vqmt $original $processed $height $width $framesnum 1 $targetname PSNR \n",
    "    \n",
    "    psnr    = pd.Series(pd.read_csv(psnr_file,    names=columns).head(-1).tail(-1).astype('float')[\"value\"])\n",
    "    #ssim    = pd.Series(pd.read_csv(ssim_file,    names=columns).head(-1).tail(-1).astype('float')[\"value\"])\n",
    "    #msssim  = pd.Series(pd.read_csv(msssim_file,  names=columns).head(-1).tail(-1).astype('float')[\"value\"])\n",
    "    #vifp    = pd.Series(pd.read_csv(vifp_file,    names=columns).head(-1).tail(-1).astype('float')[\"value\"])\n",
    "    \n",
    "    return {\n",
    "        \"psnr_mean\": psnr.mean(),\n",
    "        \"psnr_std\":  psnr.std(),\n",
    "        #\"ssim_mean\": ssim.mean(),\n",
    "        #\"ssim_std\":  ssim.std(),\n",
    "        #\"msssim_mean\": msssim.mean(),\n",
    "        #\"msssim_std\":  msssim.std(),\n",
    "        #\"vifp_mean\": vifp.mean(),\n",
    "        #\"vifp_std\":  vifp.std(),\n",
    "        };\n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#!cd ../../temp; ls #The directory where the logfiles are\n",
    "base_path       = \"../../\"\n",
    "plot_path       = \"../gnuplots\"\n",
    "temp_path       = base_path + \"/temp\" \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import csv\n",
    "import shutil\n",
    "\n",
    "def process_measurements(config, fails): \n",
    "    \n",
    "    test_path         = config['base_path'] + \"scripts/saves/\" + config['test'] + \"/\"\n",
    "    touched           = {}\n",
    "    flowstat_suffixes = [\"\",\"2\",\"3\"]\n",
    "    flowcsv_prefixes  = [\"flow1\", \"flow2\", \"flow3\"]\n",
    "\n",
    "    viewpdf           = config['base_path'] + \"/temp/statlogs.pdf\"\n",
    "    touched = {}\n",
    "    owd_dict = {'50ms':50000, '100ms':100000, '300ms': 300000, '50x100x150ms': 0}\n",
    "    \n",
    "    for dirname, dirnames, filenames in os.walk(test_path):\n",
    "        for targetdir in dirnames:\n",
    "            algorithm, count, owd, jitter = targetdir.split('_')\n",
    "            meas_path = config['base_path'] + \"scripts/saves/\" + config['test'] + \"/\" + '_'.join([algorithm, count, owd, jitter])\n",
    "\n",
    "            #Ignoring some cases if we know that they are ok.\n",
    "            if('only_algorithm' in config):\n",
    "                if(config['only_algorithm'] != algorithm):\n",
    "                    continue\n",
    "            if('only_owd' in config):\n",
    "                if(config['only_owd'] != owd):\n",
    "                    continue\n",
    "            if('only_count' in config):\n",
    "                if(config['only_count'] != count):\n",
    "                    continue\n",
    "\n",
    "            print(\"--- Operate in \" + meas_path + \" ---\")\n",
    "\n",
    "            bandwidth        = meas_path + \"/bandwidth.csv\"\n",
    "            veth0_stats      = meas_path + \"/veth0_stats.csv\"\n",
    "            rmcat_tmplogs    = meas_path + \"/rmcat_tmplogs.csv\"\n",
    "            plotfile         = plot_path + \"/\" + config['test'] + \".plot\"\n",
    "            outplot          = meas_path + \"/\" + '_'.join([config['test'],algorithm,owd,jitter]) + \".pdf\"\n",
    "            gnuplot_params   = \"\"\n",
    "            bytefactor       = 28 if algorithm == \"fractal\" else 20\n",
    "\n",
    "            # print(\"Bytefactor: \" + str(bytefactor))\n",
    "\n",
    "            for flowcounter in range(0, config['flownum']):\n",
    "\n",
    "                stat_suffix      = flowstat_suffixes[flowcounter]\n",
    "\n",
    "                snd_statlogs     = meas_path + \"/snd_statlogs\"    + stat_suffix + \".csv\"\n",
    "                rcv_statlogs     = meas_path + \"/rcv_statlogs\"    + stat_suffix + \".csv\"\n",
    "                snd_packetlogs   = meas_path + \"/snd_packetlogs\"  + stat_suffix + \".csv\"\n",
    "                rcv_packetlogs   = meas_path + \"/rcv_packetlogs\"  + stat_suffix + \".csv\"\n",
    "                merged_statlogs  = meas_path + \"/merged_statlogs\" + stat_suffix + \".csv\"  \n",
    "                delta_statlogs   = meas_path + \"/delta_statlogs\"  + stat_suffix + \".csv\"\n",
    "                rmcat_statlogs   = meas_path + \"/rmcat_statlogs\"  + stat_suffix + \".csv\"\n",
    "                evalfile         = meas_path + \"/evaluation\"      + stat_suffix + \".csv\"\n",
    "\n",
    "                gnuplot_params  += \"-e statlogs=\\'\" + rmcat_statlogs + \"\\' \"\n",
    "\n",
    "                #Made merged statlogs\n",
    "                try:\n",
    "                    make_merged_packetlogs(snd_packetlogs, rcv_packetlogs, merged_statlogs)\n",
    "                    !./../../make_delta_statlogs $merged_statlogs $delta_statlogs > log.txt\n",
    "                except:\n",
    "                    fails.append({\"algorithm\": algorithm, \"owd\": owd, \"jitter\":jitter, \"count\": count});\n",
    "                    print(\"ERROR: \" + meas_path + \" does not have files\")\n",
    "                    continue\n",
    "\n",
    "                make_path_statlogs(veth0_stats, \n",
    "                #              targetsfile, \n",
    "                               snd_statlogs, \n",
    "                               rcv_statlogs, \n",
    "                               delta_statlogs, \n",
    "                               rmcat_statlogs,\n",
    "                               bytefactor)\n",
    "\n",
    "                !sed '1d' $rmcat_statlogs > $rmcat_tmplogs; mv $rmcat_tmplogs $rmcat_statlogs\n",
    "\n",
    "                flowcsv_prefix = flowcsv_prefixes[flowcounter]\n",
    "\n",
    "                eval_result = evaluate_statslog(snd_statlogs,\n",
    "                                                snd_packetlogs,\n",
    "                                                rcv_statlogs, \n",
    "                                                rcv_packetlogs, \n",
    "                                                veth0_stats, \n",
    "                                                delta_statlogs, \n",
    "                                                config['min_samplingnum'],\n",
    "                                                algorithm,\n",
    "                                                config['flownum'],\n",
    "                                                bytefactor)\n",
    "\n",
    "                psnr_resultfile = meas_path + \"/vqmt_psnr.csv\"\n",
    "                if(os.path.isfile( psnr_resultfile )) : \n",
    "                    psnr = pd.Series(pd.read_csv(psnr_resultfile, names=[\"frame\",\"value\"]).head(-1).tail(-1).astype('float')[\"value\"])\n",
    "                    eval_result[\"psnr_mean\"] = psnr.mean()\n",
    "                    eval_result[\"psnr_std\"]  = psnr.std()\n",
    "\n",
    "                resultfile = test_path + '_'.join([flowcsv_prefix, algorithm, owd, jitter]) +\".csv\"\n",
    "                try:\n",
    "                    csv_append(touched, resultfile, eval_result)\n",
    "                except AttributeError:\n",
    "                    fails.append({\"algorithm\": algorithm, \"owd\": owd, \"jitter\":jitter, \"count\": count});\n",
    "                    print(\"ERROR: \" + meas_path + \" does not have the sufficient amount of sampling\")\n",
    "                    continue\n",
    "            \n",
    "            path_delay = owd_dict[owd]\n",
    "            if(config['flownum'] == 1):\n",
    "                !gnuplot -e \"statlogs='$meas_path/rmcat_statlogs.csv'\" \\\n",
    "                     -e \"output_file='$outplot'\" \\\n",
    "                     -e \"path_delay='$path_delay'\"  \\\n",
    "                     -e \"algorithm='$algorithm'\" \\\n",
    "                     $plotfile\n",
    "            elif(config['flownum'] == 2):\n",
    "                !gnuplot -e \"statlogs='$meas_path/rmcat_statlogs.csv'\" \\\n",
    "                         -e \"statlogs2='$meas_path/rmcat_statlogs2.csv'\" \\\n",
    "                         -e \"output_file='$outplot'\" \\\n",
    "                         -e \"path_delay='$path_delay'\" \\\n",
    "                        -e \"algorithm='$algorithm'\" \\\n",
    "                         $plotfile\n",
    "            elif(config['flownum'] == 3):\n",
    "                !gnuplot -e \"statlogs='$meas_path/rmcat_statlogs.csv'\" \\\n",
    "                         -e \"statlogs2='$meas_path/rmcat_statlogs2.csv'\" \\\n",
    "                         -e \"statlogs3='$meas_path/rmcat_statlogs3.csv'\" \\\n",
    "                         -e \"output_file='$outplot'\" \\\n",
    "                        -e \"path_delay='$path_delay'\" \\\n",
    "                         -e \"algorithm='$algorithm'\" $plotfile\n",
    "         \n",
    "    algorithms = [\"fractal\", \"scream\"]\n",
    "    owds       = [\"50ms\", \"100ms\", \"300ms\",\"50x100x150ms\"]\n",
    "    jitters    = [\"0ms\"]\n",
    "\n",
    "    with open(test_path + \"all.csv\", 'wb') as csvfile:\n",
    "        writeCSV = csv.writer(csvfile)\n",
    "        for alg in algorithms:\n",
    "            for owd in owds:\n",
    "                for jitter in jitters:\n",
    "                    for flowcounter in range(0, config['flownum']):\n",
    "                        flow_prefix = flowcsv_prefixes[flowcounter]\n",
    "                        csv_path = test_path + '_'.join([flow_prefix, alg, owd, jitter]) + \".csv\"\n",
    "                        if(os.path.isfile(csv_path) == False):\n",
    "                            continue\n",
    "\n",
    "                        #An initiation that a directory for 1 to 10 atuomatically created   \n",
    "                        #directory = test_path + '_'.join([alg,owd,])\n",
    "                        #os.path.isdir(\"bob\")\n",
    "                        \n",
    "                        start = 0\n",
    "                        with open(csv_path) as csvfile:\n",
    "                            readCSV = csv.reader(csvfile, delimiter=',')\n",
    "                            info = \" | \".join([\"algorithm: \" + alg, \n",
    "                                               \"owd: \"       + owd, \n",
    "                                               \"jitter: \"    + jitter, \n",
    "                                               \"flow: \"      + flow_prefix])\n",
    "                            writeCSV.writerow([info])\n",
    "                            for row in readCSV:\n",
    "                                writeCSV.writerow(row)\n",
    "                                start+=1\n",
    "                            for emptyRow in range(start, 11):\n",
    "                                writeCSV.writerow('')\n",
    "                            print(info + \" processed, \" + str(11 - start) + \" row is added as empty\")\n",
    "                        \n",
    "    print(\"Process for \" + config['test'] + \" is done\")\n",
    "\n",
    "def onlyplotting(config): \n",
    "    test_path         = config['base_path'] + \"scripts/saves/\" + config['test'] + \"/\"\n",
    "    touched           = {}\n",
    "    flowstat_suffixes = [\"\",\"2\",\"3\"]\n",
    "    flowcsv_prefixes  = [\"flow1\", \"flow2\", \"flow3\"]\n",
    "\n",
    "    viewpdf           = config['base_path'] + \"/temp/statlogs.pdf\"\n",
    "    touched = {}\n",
    "    owd_dict = {'50ms':50000, '100ms':100000, '300ms': 300000, '50x100x150ms': 0}\n",
    "    \n",
    "    for dirname, dirnames, filenames in os.walk(test_path):\n",
    "        for targetdir in dirnames:\n",
    "            algorithm, count, owd, jitter = targetdir.split('_')\n",
    "            meas_path = config['base_path'] + \"scripts/saves/\" + config['test'] + \"/\" + '_'.join([algorithm, count, owd, jitter])\n",
    "            \n",
    "            rmcat_tmplogs    = meas_path + \"/rmcat_tmplogs.csv\"\n",
    "            plotfile         = plot_path + \"/\" + config['test'] + \".plot\"\n",
    "            outplot          = meas_path + \"/\" + '_'.join([config['test'],algorithm,owd,jitter]) + \".pdf\"\n",
    "            path_delay       = owd_dict[owd]\n",
    "            \n",
    "            if(config['flownum'] == 1):\n",
    "                !gnuplot -e \"statlogs='$meas_path/rmcat_statlogs.csv'\" \\\n",
    "                     -e \"output_file='$outplot'\" \\\n",
    "                     -e \"path_delay='$path_delay'\"  \\\n",
    "                    -e \"algorithm='$algorithm'\" \\\n",
    "                     $plotfile\n",
    "            elif(config['flownum'] == 2):\n",
    "                !gnuplot -e \"statlogs='$meas_path/rmcat_statlogs.csv'\" \\\n",
    "                         -e \"statlogs2='$meas_path/rmcat_statlogs2.csv'\" \\\n",
    "                         -e \"output_file='$outplot'\" \\\n",
    "                         -e \"path_delay='$path_delay'\" \\\n",
    "                        -e \"algorithm='$algorithm'\" \\\n",
    "                        $plotfile\n",
    "            elif(config['flownum'] == 3):\n",
    "                !gnuplot -e \"statlogs='$meas_path/rmcat_statlogs.csv'\" \\\n",
    "                         -e \"statlogs2='$meas_path/rmcat_statlogs2.csv'\" \\\n",
    "                         -e \"statlogs3='$meas_path/rmcat_statlogs3.csv'\" \\\n",
    "                         -e \"output_file='$outplot'\" \\\n",
    "                        -e \"path_delay='$path_delay'\" \\\n",
    "                         -e \"algorithm='$algorithm'\" \\\n",
    "                         $plotfile\n",
    "            \n",
    "            print(\"Plotting \" + meas_path)\n",
    "            \n",
    "    \n",
    "\n",
    "def measurement_repair(failed, src_path, dst_path):\n",
    "    for dirname, dirnames, filenames in os.walk(src_path):\n",
    "        for targetdir in dirnames:\n",
    "            algorithm, count, owd, jitter = targetdir.split('_')\n",
    "            if(failed['algorithm'] != algorithm or\n",
    "               failed['owd']       != owd       or\n",
    "               failed['jitter']    != jitter\n",
    "              ):\n",
    "                continue\n",
    "\n",
    "            meas_dir = '_'.join([algorithm, failed['count'], owd, jitter])\n",
    "            print(\"Try to repair \" + dst_path + \"/\" + meas_dir + \" from \" + src_path + \"/\" + targetdir)\n",
    "            !rm $dst_path/$meas_dir/*\n",
    "            !cp $src_path/$targetdir/* $dst_path/$meas_dir\n",
    "            #!mv $src_path/$targetdir ../$src_path\n",
    "            shutil.rmtree(src_path + \"/\" + targetdir)\n",
    "            return True\n",
    "        return False\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--- Operate in ../../scripts/saves/mprtp1/fractal_0_50ms_0ms ---\n",
      "Segmentation fault (core dumped)\n",
      "Segmentation fault (core dumped)\n",
      "algorithm: fractal | owd: 50ms | jitter: 0ms | flow: flow1 processed, 9 row is added as empty\n",
      "algorithm: fractal | owd: 50ms | jitter: 0ms | flow: flow2 processed, 9 row is added as empty\n",
      "Process for mprtp1 is done\n"
     ]
    }
   ],
   "source": [
    "configs = []\n",
    "\n",
    "# - - - - - - RMCAT1 Config - - - - - - - - - -\n",
    "config = {}\n",
    "config['only_owd']          = \"50ms\"\n",
    "config['only_algorithm']    = \"fractal\"\n",
    "config['only_count']        = \"4\"\n",
    "config['base_path']         = \"../../\"                       \n",
    "config['test']              = \"rmcat1\"\n",
    "config['flownum']           = 1\n",
    "config['min_samplingnum']   = 1200\n",
    "config['repair_path_src']   = \"../../measurements/\" + config[\"test\"]\n",
    "config['repair_path_dst']   = config['base_path'] + \"scripts/saves/\" + config[\"test\"]\n",
    "\n",
    "# configs.append(config) #<-UNCOMMENT THIS EVALUATE RMCAT1 MEASUREMENTS\n",
    "\n",
    "# - - - - - - RMCAT2 Config - - - - - - - - - -\n",
    "config = {}\n",
    "config['only_owd']          = \"100ms\"\n",
    "config['only_algorithm']    = \"fractal\"\n",
    "config['only_count']        = \"1\"\n",
    "config['base_path']         = \"../../\"                       \n",
    "config['test']              = \"rmcat2\"\n",
    "config['flownum']           = 2\n",
    "config['min_samplingnum']   = 1200\n",
    "config['repair_path_src']   = \"../../measurements/\" + config[\"test\"]\n",
    "config['repair_path_dst']   = config['base_path'] + \"scripts/saves/\" + config[\"test\"]\n",
    "\n",
    "# configs.append(config) #<-UNCOMMENT THIS EVALUATE RMCAT2 MEASUREMENTS\n",
    "\n",
    "# - - - - - - RMCAT3 Config - - - - - - - - - -\n",
    "config = {}\n",
    "config['only_owd']          = \"100ms\"\n",
    "config['only_algorithm']    = \"scream\"\n",
    "config['only_count']        = \"1\"\n",
    "config['base_path']         = \"../../\"                       \n",
    "config['test']              = \"rmcat3\"\n",
    "config['flownum']           = 2\n",
    "config['min_samplingnum']   = 1200\n",
    "config['repair_path_src']   = \"../../measurements/\" + config[\"test\"]\n",
    "config['repair_path_dst']   = config['base_path'] + \"scripts/saves/\" + config[\"test\"]\n",
    "\n",
    "# configs.append(config) #<-UNCOMMENT THIS EVALUATE RMCAT3 MEASUREMENTS\n",
    "\n",
    "# - - - - - - RMCAT4 Config - - - - - - - - - -\n",
    "config = {}\n",
    "config['only_owd']          = \"100ms\"\n",
    "config['only_algorithm']    = \"scream\"\n",
    "config['only_count']        = \"9\"\n",
    "config['base_path']         = \"../../\"                       \n",
    "config['test']              = \"rmcat4\"\n",
    "config['flownum']           = 3\n",
    "config['min_samplingnum']   = 1200\n",
    "config['repair_path_src']   = \"../../measurements/\" + config[\"test\"]\n",
    "config['repair_path_dst']   = config['base_path'] + \"scripts/saves/\" + config[\"test\"]\n",
    "\n",
    "# configs.append(config) #<-UNCOMMENT THIS EVALUATE RMCAT4 MEASUREMENTS\n",
    "\n",
    "# - - - - - - RMCAT5 Config - - - - - - - - - -\n",
    "config = {}\n",
    "config['only_owd']          = \"100ms\"\n",
    "config['only_algorithm']    = \"scream\"\n",
    "config['only_count']        = \"1\"\n",
    "config['base_path']         = \"../../\"                       \n",
    "config['test']              = \"rmcat5\"\n",
    "config['flownum']           = 3\n",
    "config['min_samplingnum']   = 2750\n",
    "config['repair_path_src']   = \"../../measurements/\" + config[\"test\"]\n",
    "config['repair_path_dst']   = config['base_path'] + \"scripts/saves/\" + config[\"test\"]\n",
    "\n",
    "# configs.append(config) #<-UNCOMMENT THIS EVALUATE RMCAT5 MEASUREMENTS\n",
    "\n",
    "# - - - - - - RMCAT6 Config - - - - - - - - - -\n",
    "config = {}\n",
    "config['only_owd']          = \"100ms\"\n",
    "config['only_algorithm']    = \"scream\"\n",
    "config['only_count']        = \"1\"\n",
    "config['base_path']         = \"../../\"                       \n",
    "config['test']              = \"rmcat6\"\n",
    "config['flownum']           = 1\n",
    "config['min_samplingnum']   = 1200\n",
    "config['repair_path_src']   = \"../../measurements/\" + config[\"test\"]\n",
    "config['repair_path_dst']   = config['base_path'] + \"scripts/saves/\" + config[\"test\"]\n",
    "\n",
    "# configs.append(config) #<-UNCOMMENT THIS EVALUATE RMCAT6 MEASUREMENTS\n",
    "\n",
    "# - - - - - - RMCAT7 Config - - - - - - - - - -\n",
    "config = {}\n",
    "config['only_owd']          = \"50ms\"\n",
    "config['only_algorithm']    = \"fractal\"\n",
    "# config['only_count']        = \"3\"\n",
    "config['base_path']         = \"../../\"                       \n",
    "config['test']              = \"rmcat7\"\n",
    "config['flownum']           = 1\n",
    "config['min_samplingnum']   = 2900\n",
    "config['repair_path_src']   = \"../../measurements/\" + config[\"test\"]\n",
    "config['repair_path_dst']   = config['base_path'] + \"scripts/saves/\" + config[\"test\"]\n",
    "\n",
    "#configs.append(config) #<-UNCOMMENT THIS EVALUATE RMCAT7 MEASUREMENTS\n",
    "\n",
    "# - - - - - - MPRTP1 Config - - - - - - - - - -\n",
    "\n",
    "config = {}\n",
    "# config['only_owd']          = \"50ms\"\n",
    "config['only_algorithm']    = \"fractal\"\n",
    "# config['only_count']        = \"3\"\n",
    "config['base_path']         = \"../../\"                       \n",
    "config['test']              = \"mprtp1\"\n",
    "config['flownum']           = 2\n",
    "config['min_samplingnum']   = 1400\n",
    "config['repair_path_src']   = \"../../measurements/\" + config[\"test\"]\n",
    "config['repair_path_dst']   = config['base_path'] + \"scripts/saves/\" + config[\"test\"]\n",
    "\n",
    "configs.append(config) #<-UNCOMMENT THIS EVALUATE MPRTP1 MEASUREMENTS\n",
    "    \n",
    "for config in configs:\n",
    "    run = True\n",
    "    already_tried = []\n",
    "    while(run):\n",
    "        fails = []\n",
    "        process_measurements(config, fails)\n",
    "        run = False\n",
    "        for failed in fails:\n",
    "            element_hash = '_'.join([failed['algorithm'], failed['owd'], failed['jitter'], failed['count']])\n",
    "            if(element_hash in already_tried):\n",
    "                continue\n",
    "            print(\"Searching for failed: \" + ', '.join([failed['algorithm'], failed['owd'], failed['jitter'], failed['count'] ]))\n",
    "            if(measurement_repair(failed, config['repair_path_src'], config['repair_path_dst'])):\n",
    "                run = True\n",
    "            already_tried.append(element_hash)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "for config in configs:\n",
    "    onlyplotting(config)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#Save measurement results\n",
    "import csv\n",
    "\n",
    "# test      = \"rmcat1\"\n",
    "# test      = \"rmcat2\"\n",
    "# test      = \"rmcat3\"\n",
    "# test      = \"rmcat4\"\n",
    "# test      = \"rmcat5\"\n",
    "test      = \"rmcat6\"\n",
    "algorithm = \"fractal\"\n",
    "# algorithm = \"scream\"\n",
    "count     = \"1\"\n",
    "owd       = \"300ms\" \n",
    "# owd      = \"100ms\" \n",
    "# owd      = \"50ms\" \n",
    "owd      = \"50x100x150ms\"\n",
    "jitter    = \"0ms\"\n",
    "target    = \"_\".join([algorithm, count, owd, jitter])\n",
    "\n",
    "#For analyzing videoresult\n",
    "# evaluate_videofiles(base_path + \"produced.yuv\", \n",
    "#                     base_path + \"consumed.yuv\", \n",
    "#                     352,  #width\n",
    "#                     288,  #height\n",
    "#                     2000, #frames\n",
    "#                     temp_path + \"/vqmt\")\n",
    "\n",
    "save_destination = base_path + \"scripts/saves/\" + test + \"/\" + target\n",
    "if(os.path.exists(save_destination) == False): \n",
    "    !mkdir $save_destination\n",
    "!cp $temp_path/* $save_destination\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#selected measurement graphs\n",
    "selections = []\n",
    "src_path   = \"../saves/\" \n",
    "dst_path   = \"../../selected_figures\" \n",
    "def get_selection(test, algorithm, count, owd, jitter):\n",
    "    return src_path + test + \"/\" + '_'.join([algorithm, str(count), owd, jitter]) + \"/\" + '_'.join([test, algorithm, owd, jitter]) + \".pdf\"\n",
    "\n",
    "def get_path(test, algorithm, count, owd, jitter):\n",
    "    return src_path + test + \"/\" + '_'.join([algorithm, str(count), owd, jitter])\n",
    "\n",
    "\n",
    "test = \"rmcat1\"\n",
    "selections.append(get_selection(test, \"fractal\", 5, \"50ms\",  \"0ms\"))\n",
    "selections.append(get_selection(test, \"fractal\", 9, \"100ms\", \"0ms\"))\n",
    "selections.append(get_selection(test, \"fractal\", 6, \"300ms\", \"0ms\"))\n",
    "selections.append(get_selection(test, \"scream\",  1, \"50ms\",  \"0ms\"))\n",
    "selections.append(get_selection(test, \"scream\",  9, \"100ms\", \"0ms\"))\n",
    "selections.append(get_selection(test, \"scream\",  9, \"300ms\", \"0ms\"))\n",
    "\n",
    "test = \"rmcat2\"\n",
    "selections.append(get_selection(test, \"fractal\", 5, \"50ms\",  \"0ms\"))\n",
    "selections.append(get_selection(test, \"fractal\", 3, \"100ms\", \"0ms\"))\n",
    "selections.append(get_selection(test, \"fractal\", 8, \"300ms\", \"0ms\"))\n",
    "selections.append(get_selection(test, \"scream\",  9, \"50ms\",  \"0ms\"))\n",
    "selections.append(get_selection(test, \"scream\",  6, \"100ms\", \"0ms\"))\n",
    "selections.append(get_selection(test, \"scream\",  3, \"300ms\", \"0ms\"))\n",
    "\n",
    "test = \"rmcat3\"\n",
    "selections.append(get_selection(test, \"fractal\", 8, \"50ms\",  \"0ms\"))\n",
    "selections.append(get_selection(test, \"fractal\", 7, \"100ms\", \"0ms\"))\n",
    "selections.append(get_selection(test, \"fractal\", 3, \"300ms\", \"0ms\"))\n",
    "selections.append(get_selection(test, \"scream\",  1, \"50ms\",  \"0ms\"))\n",
    "selections.append(get_selection(test, \"scream\",  1, \"100ms\", \"0ms\"))\n",
    "selections.append(get_selection(test, \"scream\",  1, \"300ms\", \"0ms\"))\n",
    "\n",
    "\n",
    "test = \"rmcat4\"\n",
    "selections.append(get_selection(test, \"fractal\", 7, \"50ms\",  \"0ms\"))\n",
    "selections.append(get_selection(test, \"fractal\", 6, \"100ms\", \"0ms\"))\n",
    "selections.append(get_selection(test, \"fractal\", 1, \"300ms\", \"0ms\"))\n",
    "selections.append(get_selection(test, \"scream\",  7, \"50ms\",  \"0ms\"))\n",
    "selections.append(get_selection(test, \"scream\",  6, \"100ms\", \"0ms\"))\n",
    "selections.append(get_selection(test, \"scream\",  1, \"300ms\", \"0ms\"))\n",
    "\n",
    "test = \"rmcat5\"\n",
    "selections.append(get_selection(test, \"fractal\", 10, \"50x100x150ms\",  \"0ms\"))\n",
    "selections.append(get_selection(test, \"scream\",  5, \"50x100x150ms\",  \"0ms\"))\n",
    "\n",
    "\n",
    "test = \"rmcat6\"\n",
    "selections.append(get_selection(test, \"fractal\", 8, \"50ms\",  \"0ms\"))\n",
    "selections.append(get_selection(test, \"fractal\", 6, \"100ms\", \"0ms\"))\n",
    "selections.append(get_selection(test, \"fractal\", 7, \"300ms\", \"0ms\"))\n",
    "selections.append(get_selection(test, \"scream\",  9, \"50ms\",  \"0ms\"))\n",
    "selections.append(get_selection(test, \"scream\",  3, \"100ms\", \"0ms\"))\n",
    "selections.append(get_selection(test, \"scream\",  1, \"300ms\", \"0ms\"))\n",
    "\n",
    "\n",
    "test = \"rmcat7\"\n",
    "selections.append(get_selection(test, \"fractal\", 4, \"50ms\",  \"0ms\"))\n",
    "selections.append(get_selection(test, \"fractal\", 6, \"100ms\", \"0ms\"))\n",
    "selections.append(get_selection(test, \"fractal\", 8, \"300ms\", \"0ms\"))\n",
    "selections.append(get_selection(test, \"scream\",  1, \"50ms\",  \"0ms\"))\n",
    "selections.append(get_selection(test, \"scream\",  6, \"100ms\", \"0ms\"))\n",
    "selections.append(get_selection(test, \"scream\",  7, \"300ms\", \"0ms\"))\n",
    "\n",
    "for src in selections:\n",
    "    !cp $src $dst_path\n",
    "\n",
    "    \n",
    "    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "problematics = []\n",
    "\n",
    "def add_problems(test, algorithm, owd, jitter, counts):\n",
    "    for c in counts:\n",
    "        problematics.append(get_path(test, algorithm, c, owd,  jitter)) \n",
    "\n",
    "test = \"rmcat7\"\n",
    "# add_problems(test, \"fractal\", \"50x100x150ms\", \"0ms\", [3])\n",
    "# add_problems(test, \"scream\", \"50x100x150ms\", \"0ms\", [4])\n",
    "# add_problems(test, \"fractal\", \"50ms\", \"0ms\", [1,2])\n",
    "# add_problems(test, \"fractal\", \"100ms\", \"0ms\", [8])\n",
    "# add_problems(test, \"fractal\", \"300ms\", \"0ms\", [4,6,7])\n",
    "# add_problems(test, \"scream\", \"50ms\", \"0ms\", [3,1])\n",
    "# add_problems(test, \"scream\", \"100ms\", \"0ms\", [4,3])\n",
    "# add_problems(test, \"scream\", \"300ms\", \"0ms\", [7])\n",
    "\n",
    "for dst in problematics:\n",
    "    print(\"Delete files at: \" + dst)\n",
    "    !rm $dst/*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from wand.image import Image as WImage\n",
    "from io import BytesIO\n",
    "from IPython.display import display, Image\n",
    "\n",
    "test      = \"rmcat1\"\n",
    "# test      = \"rmcat2\"\n",
    "# test      = \"rmcat3\"\n",
    "# test      = \"rmcat4\"\n",
    "# test      = \"rmcat5\"\n",
    "# test      = \"rmcat6\"\n",
    "# test      = \"rmcat7\"\n",
    "algorithm = \"fractal\"\n",
    "# algorithm = \"scream\"\n",
    "# owd       = \"50x100x150ms\"\n",
    "owd       = \"50ms\"\n",
    "# owd       = \"100ms\"\n",
    "# owd       = \"300ms\"\n",
    "jitter    = \"0ms\"\n",
    " \n",
    "\n",
    "for counter in range(1, 11):\n",
    "    try:\n",
    "        directory = test + \"/\" + '_'.join([algorithm, str(counter), owd, jitter])\n",
    "        pdf       = '_'.join([test, algorithm, owd, jitter]) + \".pdf\"\n",
    "        img       = WImage(filename='../saves/' + directory +'/' + pdf)\n",
    "        display(img)\n",
    "        print(\"Figure \" + str(counter) + \". \" + ' '.join([algorithm, str(counter), owd, jitter]))\n",
    "    except:\n",
    "        print(\"ERROR happened at \" + ' '.join([algorithm, str(counter), owd, jitter]))\n",
    "        continue\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
