{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 119,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from shutil import copyfile\n",
    "import os.path\n",
    "\n",
    "def get_packetlog_columns():\n",
    "    return [ 'extended_seq',\n",
    "             'tracked_seq',\n",
    "             'state',\n",
    "             'tracked_ntp',\n",
    "             'marker',\n",
    "             'header_size',\n",
    "             'payload_size',\n",
    "             'timestamp',\n",
    "             'payload_type',\n",
    "             'played_out'];\n",
    "\n",
    "def get_delta_packetlog_columns():\n",
    "    return [\"owd\",\n",
    "            \"BiF\",\n",
    "            \"playout_delay\"];\n",
    "\n",
    "def get_statslog_columns():\n",
    "    return [ 'rcved_tot_packets',\n",
    "             'rcved_tot_bytes',\n",
    "             'rcved_acc_bytes',\n",
    "             'rcved_acc_packets',                             \n",
    "\n",
    "             'lost_tot_packets',\n",
    "             'lost_tot_bytes',\n",
    "             'lost_acc_bytes',\n",
    "             'lost_acc_packets',\n",
    "\n",
    "             'discarded_tot_packets',\n",
    "             'discarded_tot_bytes',\n",
    "             'discarded_acc_bytes',\n",
    "             'discarded_acc_packets',\n",
    "\n",
    "             'corrupted_tot_packets',\n",
    "             'corrupted_tot_bytes',\n",
    "             'corrupted_acc_bytes',\n",
    "             'corrupted_acc_packets',\n",
    "\n",
    "             'repaired_tot_packets',\n",
    "             'repaired_tot_bytes',\n",
    "             'repaired_acc_bytes',\n",
    "             'repaired_acc_packets',\n",
    "\n",
    "             'fec_tot_packets',\n",
    "             'fec_tot_bytes',\n",
    "             'fec_acc_bytes',\n",
    "             'fec_acc_packets',\n",
    "            ];\n",
    "def get_veth_columns():\n",
    "    return [\"tcp_bytes\",\n",
    "            \"tcp_packets\",\n",
    "            \"tcp_flowsnum\",\n",
    "            \"bw_forward\", \n",
    "            \"bw_backward\"];\n",
    "\n",
    "def get_bytefactor():\n",
    "\n",
    "    #bytes with only rtp:\n",
    "    return 20\n",
    "\n",
    "    #bytes with mprtp and abs time header ext\n",
    "    #return 28"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from shutil import copyfile\n",
    "import os.path\n",
    "import pandas as pd\n",
    "import datetime\n",
    "\n",
    "#Make owd_statlogs\n",
    "def make_merged_packetlogs(snd_packetlogs, rcv_packetlogs, merged_packetlogs_path):\n",
    "    packetlog_columns     = get_packetlog_columns()\n",
    "    snd_packets = pd.read_csv(snd_packetlogs, names=packetlog_columns)\n",
    "    rcv_packets = pd.read_csv(rcv_packetlogs, names=packetlog_columns)\n",
    "    \n",
    "    result = pd.merge(snd_packets, rcv_packets, on='extended_seq', \\\n",
    "                           suffixes=['_snd','_rcv'])\n",
    "\n",
    "    result.to_csv(merged_packetlogs_path, sep=',')\n",
    "    \n",
    "    #Make owd_statlogs\n",
    "def make_path_statlogs(veth_logfile, \n",
    "#                        targets_logfile,\n",
    "                       snd_statlogfile, \n",
    "                       rcv_statlogfile,\n",
    "                       delta_packetlogfile, \n",
    "                       result_filepath):\n",
    "    \n",
    "    bytefactor          = get_bytefactor()\n",
    "    statlog_columns     = get_statslog_columns()\n",
    "    \n",
    "    delta_columns = get_delta_packetlog_columns()\n",
    "   \n",
    "    #targets_stats = pd.read_csv(targets_logfile,     names=[\"target\"])\n",
    "    veth_stats    = pd.read_csv(veth_logfile,        names=get_veth_columns())\n",
    "    snd_stats     = pd.read_csv(snd_statlogfile,     names=statlog_columns)\n",
    "    rcv_stats     = pd.read_csv(rcv_statlogfile,     names=statlog_columns)\n",
    "    packet_dstats = pd.read_csv(delta_packetlogfile, names=delta_columns)\n",
    "    \n",
    "    sending_rates = snd_stats.apply(lambda record: record['rcved_acc_packets'] * bytefactor + \n",
    "                                    record['rcved_acc_bytes'], axis=1)\n",
    "    # sending_rates = snd_stats.apply(lambda record: record['rcved_acc_bytes'], axis=1)\n",
    "    \n",
    "    fec_rates     = snd_stats.apply(lambda record: record['fec_acc_packets'] * bytefactor + \n",
    "                                    record['fec_acc_bytes'], axis=1)\n",
    "    \n",
    "    ffre          = rcv_stats.apply(lambda record:  \n",
    "                                    record['repaired_acc_packets'] / (record['repaired_acc_packets'] + record['lost_acc_packets']) if 0 < record['repaired_acc_packets'] else 0, axis=1)\n",
    "    \n",
    "    #print(rcv_stats['lost_acc_packets'])\n",
    "    #print(rcv_stats['repaired_acc_packets'])\n",
    "    #print(packet_dstats);\n",
    "    result = pd.concat([veth_stats[\"bw_forward\"], \n",
    "                        veth_stats[\"bw_backward\"], \n",
    "                        sending_rates,\n",
    "                        fec_rates,\n",
    "                        packet_dstats['owd'],\n",
    "                        packet_dstats['BiF'],\n",
    "                        ffre,\n",
    "#                         targets_stats[\"target\"],\n",
    "                        packet_dstats['playout_delay'],\n",
    "                        rcv_stats[\"lost_acc_packets\"],\n",
    "                        rcv_stats[\"repaired_acc_packets\"],\n",
    "                        veth_stats[\"tcp_flowsnum\"],\n",
    "                        veth_stats[\"tcp_bytes\"],\n",
    "                        veth_stats[\"tcp_packets\"],                        \n",
    "                       ], \n",
    "                       axis=1)\n",
    "#    print(result)\n",
    "    result.to_csv(result_filepath, sep=',')\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 120,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from shutil import copyfile\n",
    "import os.path\n",
    "import pandas as pd\n",
    "import datetime\n",
    "\n",
    "def evaluate_statslog(statslog_file, \n",
    "                      packetlogs_file, \n",
    "                      veth_logfile, \n",
    "                      dpck_logfile,\n",
    "                      sampling_num):\n",
    "    \n",
    "    bytefactor        = get_bytefactor() \n",
    "    packetlog_columns = get_packetlog_columns()\n",
    "    statlog_table     = pd.read_csv(statslog_file, names=get_statslog_columns())\n",
    "    veth_table        = pd.read_csv(veth_logfile,  names=[\"tcps\",\"bw\"])\n",
    "    dpcklog_table     = pd.read_csv(dpck_logfile,  names=get_delta_packetlog_columns()).head(sampling_num + 1).tail(sampling_num)\n",
    "    goodput_helper    = pd.Series(((statlog_table[\"rcved_acc_bytes\"] - statlog_table[\"discarded_acc_bytes\"]) + \n",
    "                                 (statlog_table[\"rcved_acc_packets\"] - statlog_table[\"discarded_acc_packets\"]) * bytefactor ) / 125)\n",
    "    \n",
    "    owdmean_helper    = pd.Series(dpcklog_table[\"owd\"] / 1000)\n",
    "\n",
    "    #shaping\n",
    "    available_sampling_num = len(statlog_table[\"rcved_acc_packets\"])\n",
    "    if(available_sampling_num < sampling_num) : \n",
    "        print(\"Required sampling number is \" + str(sampling_num) + \n",
    "              \" available: \" + str(available_sampling_num) + \" at \" + statslog_file)\n",
    "        return\n",
    "    \n",
    "    print(\"Required sampling num is \" + str(sampling_num) + \" : \" + str(len(statlog_table[\"rcved_acc_packets\"])))\n",
    "    \n",
    "    statlog_table     = statlog_table.head(sampling_num + 1).tail(sampling_num)\n",
    "    veth_table        = veth_table.head(sampling_num + 1).tail(sampling_num)\n",
    "    dpcklog_table     = dpcklog_table.head(sampling_num + 1).tail(sampling_num)\n",
    "    goodput_helper    = goodput_helper.head(sampling_num + 1).tail(sampling_num)\n",
    "\n",
    "    index = sampling_num - 1\n",
    "    rcved_packets     = statlog_table.iloc[index][\"rcved_tot_packets\"]\n",
    "    repaired_packets  = statlog_table.iloc[index][\"repaired_tot_packets\"]\n",
    "    rcved_packets     = statlog_table.iloc[index][\"rcved_tot_packets\"]\n",
    "    rcved_bytes       = statlog_table.iloc[index][\"rcved_tot_bytes\"]\n",
    "    discarded_packets = statlog_table.iloc[index][\"discarded_tot_packets\"]\n",
    "    discarded_bytes   = statlog_table.iloc[index][\"discarded_tot_bytes\"]\n",
    "    lost_packets      = statlog_table.iloc[index][\"lost_tot_packets\"]\n",
    "    received_packets  = statlog_table.iloc[index][\"rcved_tot_packets\"]\n",
    "    lost_rate         = float(lost_packets) / float(received_packets + lost_packets)\n",
    "    packetlogs_table  = pd.read_csv(packetlogs_file, names=packetlog_columns).head(received_packets + 1).tail(received_packets)\n",
    "    \n",
    "\n",
    "    frame_is_lost   = False\n",
    "    lost_frames     = 0\n",
    "    received_frames = 0    \n",
    "    last_timestamp  = 0;\n",
    "    \n",
    "    for index, row in packetlogs_table.iterrows():\n",
    "        if(row[\"timestamp\"] != 0):\n",
    "            last_timestamp = row[\"timestamp\"]\n",
    "            break\n",
    "\n",
    "    for index, row in packetlogs_table.iterrows():\n",
    "        if(row[\"timestamp\"] == 0 or row[\"state\"] != 1):\n",
    "            frame_is_lost = True;\n",
    "            continue;\n",
    "        if(row[\"timestamp\"] != last_timestamp):\n",
    "            last_timestamp = row[\"timestamp\"]\n",
    "            if(frame_is_lost):\n",
    "                lost_frames += 1\n",
    "            else: \n",
    "                received_frames += 1\n",
    "            frame_is_lost = False;\n",
    "    repaired_rate = 0.\n",
    "    if(0 < lost_packets):\n",
    "        repaired_rate = float(repaired_packets) / float(repaired_packets + lost_packets)\n",
    "    \n",
    "    lost_frame_rate = 0;\n",
    "    if(0 < received_frames):\n",
    "        lost_frame_rate = float(lost_frames) / float(lost_frames + received_frames)\n",
    "    \n",
    "    return{\n",
    "        'repaired_packets_num':    repaired_packets,\n",
    "        'repaired_rate':           repaired_rate,\n",
    "        'lost_packet_rate':        lost_rate, \n",
    "        'goodput_mean':            goodput_helper.mean(),\n",
    "        'goodput_std':             goodput_helper.std(),\n",
    "        'lost_packets_num':        lost_packets,\n",
    "        'rcved_packets_num':       rcved_packets,\n",
    "        'lost_frames_num':         lost_frames,\n",
    "        'lost_frame_rate':         lost_frame_rate,\n",
    "        'received_frames':         received_frames,\n",
    "        'owd mean':                owdmean_helper.mean(),\n",
    "        'owd std':                 owdmean_helper.std(),\n",
    "        'received_bytes':          rcved_bytes,\n",
    "    };\n",
    "\n",
    "def evaluate_videofiles(original, processed, width, height, framesnum, targetname):\n",
    "    psnr_file   = targetname + \"_psnr.csv\"\n",
    "    ssim_file   = targetname + \"_ssim.csv\"\n",
    "    msssim_file = targetname + \"_msssim.csv\"\n",
    "    vifp_file   = targetname + \"_vifp.csv\"\n",
    "    columns     = [\"frame\",\"value\"]\n",
    "    \n",
    "    print(\"Evaluating Video\")\n",
    "    print(\"Source: \" + original +\" Processed: \" + processed + \" W: \" + str(width) + \" H: \" + str(height) + \" psnr: \" + psnr_file)\n",
    "    print(\"Framesnum: \" + str(framesnum))\n",
    "    \n",
    "    #!vqmt $original $processed $height $width $framesnum 1 $targetname PSNR SSIM MSSSIM VIFP\n",
    "    !vqmt $original $processed $height $width $framesnum 1 $targetname PSNR \n",
    "    \n",
    "    psnr    = pd.Series(pd.read_csv(psnr_file,    names=columns).head(-1).tail(-1).astype('float')[\"value\"])\n",
    "    #ssim    = pd.Series(pd.read_csv(ssim_file,    names=columns).head(-1).tail(-1).astype('float')[\"value\"])\n",
    "    #msssim  = pd.Series(pd.read_csv(msssim_file,  names=columns).head(-1).tail(-1).astype('float')[\"value\"])\n",
    "    #vifp    = pd.Series(pd.read_csv(vifp_file,    names=columns).head(-1).tail(-1).astype('float')[\"value\"])\n",
    "    \n",
    "    return {\n",
    "        \"psnr_mean\": psnr.mean(),\n",
    "        \"psnr_std\":  psnr.std(),\n",
    "        #\"ssim_mean\": ssim.mean(),\n",
    "        #\"ssim_std\":  ssim.std(),\n",
    "        #\"msssim_mean\": msssim.mean(),\n",
    "        #\"msssim_std\":  msssim.std(),\n",
    "        #\"vifp_mean\": vifp.mean(),\n",
    "        #\"vifp_std\":  vifp.std(),\n",
    "        };\n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 121,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#!cd ../../temp; ls #The directory where the logfiles are\n",
    "base_path       = \"../../\"\n",
    "plot_path       = \"../gnuplots\"\n",
    "temp_path       = base_path + \"/temp\" \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 123,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Evaluating Video\n",
      "Source: ../..//produced.yuv Processed: ../..//consumed.yuv W: 352 H: 288 psnr: ../..//temp/vqmt_psnr.csv\n",
      "Framesnum: 2000\n",
      "Time: 109.789s\n"
     ]
    }
   ],
   "source": [
    "#Save measurement results\n",
    "import csv\n",
    "\n",
    "test      = \"rmcat1\"\n",
    "algorithm = \"scream\"\n",
    "# algorithm = \"scream\"\n",
    "count     = \"1\"\n",
    "owd       = \"300ms\" \n",
    "# owd      = \"100ms\" \n",
    "# owd      = \"300ms\" \n",
    "jitter    = \"0ms\"\n",
    "target    = \"_\".join([algorithm, count, owd, jitter])\n",
    "\n",
    "evaluate_videofiles(base_path + \"produced.yuv\", \n",
    "                    base_path + \"consumed.yuv\", \n",
    "                    352,  #width\n",
    "                    288,  #height\n",
    "                    2000, #frames\n",
    "                    temp_path + \"/vqmt\")\n",
    "\n",
    "save_destination = base_path + \"scripts/saves/\" + test + \"/\" + target\n",
    "if(os.path.exists(save_destination) == False): \n",
    "    !mkdir $save_destination\n",
    "!cp $temp_path/* $save_destination\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 118,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--- Operate in ../../scripts/saves/rmcat4/fractal_1_50ms_0ms ---\n",
      "Make Delta Packet Statlogs.\n",
      "Source file:             ../../scripts/saves/rmcat4/fractal_1_50ms_0ms/merged_statlogs.csv\n",
      "Destination file:        ../../scripts/saves/rmcat4/fractal_1_50ms_0ms/delta_statlogs.csv\n",
      "Accumulation length:     1000ms\n",
      "Sampling time:           100ms\n",
      "Unit of delay:           us\n",
      "Unit of bytes in flight: byte\n",
      "Elapsed time:            0.165000s\n",
      "Nr. of processed rows:   14762\n",
      "Nr. of recorded sample:  1754\n",
      "Delta PacketStatlog is made\n",
      "Required sampling num is 1000 : 1782\n",
      "../../scripts/saves/rmcat1/flow1_fractal_50ms_0ms.csv is removed\n",
      "------ Obj to save to ../../scripts/saves/rmcat1/flow1_fractal_50ms_0ms.csv------\n",
      "Make Delta Packet Statlogs.\n",
      "Source file:             ../../scripts/saves/rmcat4/fractal_1_50ms_0ms/merged_statlogs2.csv\n",
      "Destination file:        ../../scripts/saves/rmcat4/fractal_1_50ms_0ms/delta_statlogs2.csv\n",
      "Accumulation length:     1000ms\n",
      "Sampling time:           100ms\n",
      "Unit of delay:           us\n",
      "Unit of bytes in flight: byte\n",
      "Elapsed time:            0.086000s\n",
      "Nr. of processed rows:   8299\n",
      "Nr. of recorded sample:  1605\n",
      "Delta PacketStatlog is made\n",
      "Required sampling num is 1000 : 1695\n",
      "../../scripts/saves/rmcat1/flow2_fractal_50ms_0ms.csv is removed\n",
      "------ Obj to save to ../../scripts/saves/rmcat1/flow2_fractal_50ms_0ms.csv------\n",
      "Make Delta Packet Statlogs.\n",
      "Source file:             ../../scripts/saves/rmcat4/fractal_1_50ms_0ms/merged_statlogs3.csv\n",
      "Destination file:        ../../scripts/saves/rmcat4/fractal_1_50ms_0ms/delta_statlogs3.csv\n",
      "Accumulation length:     1000ms\n",
      "Sampling time:           100ms\n",
      "Unit of delay:           us\n",
      "Unit of bytes in flight: byte\n",
      "Elapsed time:            0.196000s\n",
      "Nr. of processed rows:   22471\n",
      "Nr. of recorded sample:  1409\n",
      "Delta PacketStatlog is made\n",
      "Required sampling num is 1000 : 1496\n",
      "../../scripts/saves/rmcat1/flow3_fractal_50ms_0ms.csv is removed\n",
      "------ Obj to save to ../../scripts/saves/rmcat1/flow3_fractal_50ms_0ms.csv------\n",
      "--- Operate in ../../scripts/saves/rmcat4/fractal_2_50ms_0ms ---\n",
      "Make Delta Packet Statlogs.\n",
      "Source file:             ../../scripts/saves/rmcat4/fractal_2_50ms_0ms/merged_statlogs.csv\n",
      "Destination file:        ../../scripts/saves/rmcat4/fractal_2_50ms_0ms/delta_statlogs.csv\n",
      "Accumulation length:     1000ms\n",
      "Sampling time:           100ms\n",
      "Unit of delay:           us\n",
      "Unit of bytes in flight: byte\n",
      "Elapsed time:            0.186000s\n",
      "Nr. of processed rows:   18088\n",
      "Nr. of recorded sample:  1530\n",
      "Delta PacketStatlog is made\n",
      "Required sampling num is 1000 : 1564\n",
      "../../scripts/saves/rmcat1/flow1_fractal_50ms_0ms.csv is already touched\n",
      "------ Obj to save to ../../scripts/saves/rmcat1/flow1_fractal_50ms_0ms.csv------\n",
      "Make Delta Packet Statlogs.\n",
      "Source file:             ../../scripts/saves/rmcat4/fractal_2_50ms_0ms/merged_statlogs2.csv\n",
      "Destination file:        ../../scripts/saves/rmcat4/fractal_2_50ms_0ms/delta_statlogs2.csv\n",
      "Accumulation length:     1000ms\n",
      "Sampling time:           100ms\n",
      "Unit of delay:           us\n",
      "Unit of bytes in flight: byte\n",
      "Elapsed time:            0.116000s\n",
      "Nr. of processed rows:   10471\n",
      "Nr. of recorded sample:  1351\n",
      "Delta PacketStatlog is made\n",
      "Required sampling num is 1000 : 1462\n",
      "../../scripts/saves/rmcat1/flow2_fractal_50ms_0ms.csv is already touched\n",
      "------ Obj to save to ../../scripts/saves/rmcat1/flow2_fractal_50ms_0ms.csv------\n",
      "Make Delta Packet Statlogs.\n",
      "Source file:             ../../scripts/saves/rmcat4/fractal_2_50ms_0ms/merged_statlogs3.csv\n",
      "Destination file:        ../../scripts/saves/rmcat4/fractal_2_50ms_0ms/delta_statlogs3.csv\n",
      "Accumulation length:     1000ms\n",
      "Sampling time:           100ms\n",
      "Unit of delay:           us\n",
      "Unit of bytes in flight: byte\n",
      "Elapsed time:            0.159000s\n",
      "Nr. of processed rows:   8559\n",
      "Nr. of recorded sample:  1165\n",
      "Delta PacketStatlog is made\n",
      "Required sampling num is 1000 : 1263\n",
      "../../scripts/saves/rmcat1/flow3_fractal_50ms_0ms.csv is already touched\n",
      "------ Obj to save to ../../scripts/saves/rmcat1/flow3_fractal_50ms_0ms.csv------\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import csv\n",
    "\n",
    "# - - - - - - RMCA4 Config - - - - - - - - - -\n",
    "test              = \"rmcat4\"\n",
    "test_path         = base_path + \"scripts/saves/\" + test + \"/\"\n",
    "touched           = {}\n",
    "flownum           = 1\n",
    "\n",
    "flowstat_suffixes = [\"\",\"2\",\"3\"]\n",
    "flowcsv_prefixes  = [\"flow1\", \"flow2\", \"flow3\"]\n",
    "\n",
    "viewpdf           = base_path + \"/temp/statlogs.pdf\"\n",
    " \n",
    "\n",
    "\n",
    "for dirname, dirnames, filenames in os.walk(test_path):\n",
    "    for targetdir in dirnames:\n",
    "        algorithm, count, owd, jitter = targetdir.split('_')\n",
    "        meas_path = base_path + \"scripts/saves/rmcat4/\" + '_'.join([algorithm, count, owd, jitter])\n",
    "        print(\"--- Operate in \" + meas_path + \" ---\")\n",
    "        \n",
    "        bandwidth        = meas_path + \"/bandwidth.csv\"\n",
    "        veth0_stats      = meas_path + \"/veth0_stats.csv\"\n",
    "        rmcat_tmplogs    = meas_path + \"/rmcat_tmplogs.csv\"\n",
    "        plotfile         = plot_path + \"/\" + test + \".plot\"\n",
    "        outplot          = meas_path + \"/\" + '_'.join([algorithm,owd,jitter]) + \".pdf\"\n",
    "        gnuplot_param    = \"\"\n",
    "        \n",
    "        for flowcounter in range(0, 3):\n",
    "            \n",
    "            stat_suffix      = flowstat_suffixes[flowcounter]\n",
    "            \n",
    "            snd_statlogs     = meas_path + \"/snd_statlogs\"    + stat_suffix + \".csv\"\n",
    "            rcv_statlogs     = meas_path + \"/rcv_statlogs\"    + stat_suffix + \".csv\"\n",
    "            snd_packetlogs   = meas_path + \"/snd_packetlogs\"  + stat_suffix + \".csv\"\n",
    "            rcv_packetlogs   = meas_path + \"/rcv_packetlogs\"  + stat_suffix + \".csv\"\n",
    "            merged_statlogs  = meas_path + \"/merged_statlogs\" + stat_suffix + \".csv\"  \n",
    "            delta_statlogs   = meas_path + \"/delta_statlogs\"  + stat_suffix + \".csv\"\n",
    "            rmcat_statlogs   = meas_path + \"/rmcat_statlogs\"  + stat_suffix + \".csv\"\n",
    "            evalfile         = meas_path + \"/evaluation\"      + stat_suffix + \".csv\"\n",
    "\n",
    "            gnuplot_param   += \"-e statlogs='\" + rmcat_statlogs + \"' \"\n",
    "            \n",
    "            #Made merged statlogs\n",
    "            make_merged_packetlogs(snd_packetlogs, rcv_packetlogs, merged_statlogs)\n",
    "            !./../../make_delta_statlogs $merged_statlogs $delta_statlogs\n",
    "\n",
    "            make_path_statlogs(veth0_stats, \n",
    "            #              targetsfile, \n",
    "                           snd_statlogs, \n",
    "                           rcv_statlogs, \n",
    "                           delta_statlogs, \n",
    "                           rmcat_statlogs)\n",
    "            \n",
    "            !sed '1d' $rmcat_statlogs > $rmcat_tmplogs; mv $rmcat_tmplogs $rmcat_statlogs\n",
    "            \n",
    "            !gnuplot $gnuplot_params \\\n",
    "                 -e \"output_file='$outplot'\" \\\n",
    "                 $plotfile\n",
    "            \n",
    "            flowcsv_prefix = flowcsv_prefixes[flowcounter]\n",
    "            \n",
    "            eval_result = evaluate_statslog(rcv_statlogs, rcv_packetlogs, veth0_stats, delta_statlogs, 1000)\n",
    "     \n",
    "            resultfile = path + '_'.join([flowcsv_prefix, algorithm, owd, jitter]) +\".csv\"\n",
    "            csv_append(touched, resultfile, eval_result)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
